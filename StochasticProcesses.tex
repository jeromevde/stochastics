\documentclass{report}%
%\usepackage{STYLES/IPMstyle}
%\usepackage{placeins}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{graphicx} % needed in MAIN
\usepackage{amsthm}
 
\newcommand{\EV}[2][]{\mathbb{E}^{#1}\left[#2\right]}
%\newcommand{\qed}{\hfill\blacksquare}


\theoremstyle{definition} 
\newtheorem{exercise}{Exercise}
\newtheorem{solution}{Exercise}

\begin{document}
\begin{titlepage}
    \centering
%\includegraphics[width=3cm]{kbc_logo.jpg} % also works with logo.pdf    
\vfill
    {\bf\Huge
        Stochastic Processes in Finance\\}
%     {\bf\Huge
%        Economic Scenario Generator (ESG) - Management Summary\\}
 \vskip2cm
        February 26, 2024\\
        \vskip2cm
        P.Jacobs (M\&MR/RMO)\\
        
    \vfill
    
    \vfill
    \vfill
\end{titlepage}

\tableofcontents

\chapter{Motivation}\label{ch:Motivation}

\section {Risk Modelling}

In each of the following applications it is necessary to consider a large number of scenarios for some assets, not just at one future date but over a whole range of dates. We thus need a stochastic process simulating possible paths that may occur in the real world. The probabilities of occurrence of subsets of paths should correspond to the observed or expected distribution in the real world. Such scenarios (together with the probabilities) are sometimes called real world scenarios.

\begin{itemize}

\item Counterparty Credit Risk for OTC transactions: How much money can we lose on average if a counterparty on a portfolio of OTC contracts defaults? What amount can be lost in extreme scenarios?

\item Value at risk: How much money can we lose on a portfolio of financial instruments over a given holding period within a given confidence level? 

\item Constant Portfolio Protection Instruments: How does the protection mechanism work in certain scenarios?

\item Hedging: Test how well a hedge portfolio mimics the payoff of a real portfolio in a large number of scenarios?

\end{itemize}

\section{Derivative Valuation}

The use of stochastic processes in valuating instruments is based on the so-called fundamental asset pricing theorem. It states that we can compute the fair value of a derivative on an asset by computing its discounted expected payoff in the so-called risk neutral world, meaning that we need to change the probabilities of the stochastic payoff in a specific way. In such a risk neutral world,  the fair value of the asset itself must also equal its discounted expected value at any future time. Often the model for the asset is specified directly in the risk neutral world and calibrated such that this condition holds. In the risk neutral world, stock prices will have an expected growth rate equal to the risk free spot rate. 

Note that the payoff of more complex derivatives (such as lookback options) may depend on the value of the asset at different future times. Hence we need a stochastic process to model the asset, not just a random variable.

\chapter{Stochastic Calculus}\label{ch:StochasticCalculus}

We refer to \cite{Mikosch} for a nice and intuitive introduction to stochastic calculus. The stochastic calculus lectures \cite{Baudoin} provide a nice overview of the most important theorems. A thorough more general treatment of the theory of stochastic calculus can be found in \cite{JacodShiryaev}.		

\section{Basic Definitions}

\subsection{Random Variables}

\paragraph{$\sigma$-algebra:} A $\sigma$-algebra $\mathcal{F}$ on a set $\Omega$ is a set of subsets of $\Omega$ containing $\Omega$ and closed under taking complements and countable unions. In probability theory the elements $F$ in $\mathcal{F}$ represent events.

\paragraph{Generated $\sigma$-algebra:} The smallest $\sigma$-algebra on a set $\Omega$ containing a set $\mathcal{G}$ of subsets of $\Omega$ is called the $\sigma$-algebra generated by $\mathcal{G}$ and denoted by $\sigma(\mathcal{G}).$

\paragraph{Borel $\sigma$-algebra:} The $\sigma$-algebra generated by all open sets in $\mathbb{R}$ is called the Borel $\sigma$-algebra on $\mathbb{R}$ and denoted by $\mathcal{B}(\mathbb{R}).$ 


\paragraph{Measurable space:} A pair $(\Omega,\mathcal{F})$ where $\mathcal{F}$ is a $\sigma$-algebra on $\Omega$ is called a measurable space.

\paragraph{Measure:} A measure on a measurable space $(\Omega,\mathcal{F})$ is a function $\mu:\mathcal{F} \to  \mathbb{R}^+ \cup \{\infty \} $ such that 


\begin{eqnarray*}
\mu(\emptyset)&=&0 \\
 \mu(\bigcup_{i=1}^\infty A_i)&=&\sum_{i=1}^\infty \mu(A_i)	 
\end{eqnarray*}

for a countable set $\{ A_i \}_{i=1}^\infty$ of disjoint members of $\mathcal{F}.$

\paragraph{Semi-ring:} A semi-ring $S$ on a set $\Omega$ is a set of subsets of $\Omega$ such that

\begin{itemize}
\item $\emptyset \in \mathcal{F}$ 
\item $A \cap B \in  \mathcal{F} $ for all $A,B \in  \mathcal{F} $ 
\item for all $A,B \in  \mathcal{F} $  there exist a finite number $K_i \in \mathcal{F}$ , $1\le i \le n$, such that $A \setminus B = \cup_{i=1}^n K_i$ 
\end{itemize}

\paragraph{pre-measure:}  A pre-measure on a semi-ring $(\Omega, S)$ is a function $\mu:S \to  \mathbb{R}^+ \cup \{\infty \} $ such that 


\begin{eqnarray*}
\mu(\emptyset)&=&0 \\
 \mu(\bigcup_{i=1}^\infty A_i)&=&\sum_{i=1}^\infty \mu(A_i)	 
\end{eqnarray*}

for each countable set $\{ A_i \}_{i=1}^\infty \in S$ of disjoint members of $S$. Moreover $\mu$ is said to be $\sigma$-finite if $\Omega$ is a countable union of 
sets $A_i$ in $S$ with $\mu(A_i)<\infty$.

\paragraph{Caratheodory's extension theorem:} Let $\mu$ be a $\sigma$-finite pre-measure on a semi-ring $(\Omega, S)$. Then it uniquely extends to a measure on its generated sigma algebra $\sigma(S).$ 


\paragraph{Measure space:}  A measure space is a triple $(\Omega,\mathcal{F},\mu)$ consisting of a measurable space $(\Omega,\mathcal{F})$ and a measure $\mu$ on this space.

\paragraph{Probability measure:} A probability measure $P$ on a measurable space $(\Omega,\mathcal{F})$ is a measure on $(\Omega,\mathcal{F})$ such that $P(\Omega)=1.$

\paragraph{Probability space:} A probability space is a triple $(\Omega,\mathcal{F},P)$ consisting of a measurable space $(\Omega,\mathcal{F})$ and a probability measure $P$ on this space.




\paragraph{Measurable function:} A measurable function $f:(\Omega_1,\mathcal{F}_1)\to (\Omega_2,\mathcal{F}_2)$ from one measurable space to another is a function $f:\Omega_1 \to \Omega_2$ such that 
$$ f^{-1}(F)\in \mathcal{F}_1 \textup{ for all } F\in \mathcal{F}_2.$$

\paragraph{Lebesgue integral:} The Lebesgue integral with respect to a measure $\mu$ on a measurable space $(\Omega,\mathcal{F})$ is a functional from a subset of measurable functions $f:(\Omega,\mathcal{F})\to (\mathbb{R} \cup \{ -\infty, \infty \},\mathcal{B}(\mathbb{R}\cup \{ -\infty, \infty \}))$ having a lot of nice properties. The Lebesgue integral of $f$ is often denoted by 
\[ \int_{\Omega} f d\mu \textup{ or } \int_{\Omega} f(\omega) d\mu(\omega). \] Note however that the Lebesgue integral is not defined for every measurable function. In case the Lebesgue integral of $|f|$ is defined and not equal to $\infty$, we say that $f$ is Lebesgue integrable.
Here we do not elaborate further on the construction of the Lebesgue integral, but just give its most important properties:

\begin{itemize}
	\item the Lebesgue integral of any indicator function equals its measure:
	\[ \int_{\Omega} 1_A d\mu =\mu(A) \]
		for all $A\in \mathcal{F}.$ 
	\item linearity:
		 \[ a \int_{\Omega} f d\mu + b \int_{\Omega} g d\mu = \int_{\Omega} (af+bg) d\mu \]
		for $a,b\in \mathbb{R}$ and $f,g$ Lebesgue integrable functions.
	\item monotonicity:
		\[ \int_{\Omega} f d\mu \leq \int_{\Omega} g d\mu \] for non-negative measurable functions \[ f \leq g \]
	\item monotone convergence:
		Let $\{ f_k \}$ be a series of increasing non-negative measurable functions, then 
		\[ \lim \int_{\Omega} f_k d\mu = \int_{\Omega} \lim f_k d\mu \]
	\item Fatou's lemma:
		Let $\{ f_k \}$ be a series of non-negative measurable functions, then
		\[ \int_{\Omega} \liminf_k f_k d\mu \leq \liminf_k \int_{\Omega} f_k d\mu \]
	\item dominated convergence theorem:
		Let $\{ f_k \}$ be a series of measurable functions with pointwise limit $f$ such that for all $k$, $|f_k|\leq g$ for a Lebesgue integrable function $g$, then $f$ is Lebesgue integrable and 
		\[ \lim_k \int_{\Omega} f_k d\mu = \int_{\Omega} f d\mu \]
\end{itemize}


\paragraph{Random variable:} A random variable $X$ on a probability space $(\Omega,\mathcal{F},P)$ is a measurable function  $X:(\Omega,\mathcal{F})\to (\mathbb{R},\mathcal{B}(\mathbb{R})).$


\paragraph{Almost sure:} In a probability space  $(\Omega,\mathcal{F},P)$ an event $F \in \mathcal{F}$ is said to happen almost surely if $P(F)=1.$ 

\paragraph{$\sigma$-algebra generated by a random variable:} The $\sigma$-algebra $\{ X^{-1}(U) | U  \in \mathcal{B}(\mathbb{R}) \}$ is called the $\sigma$-algebra generated by $X$ and denoted by $\sigma(X).$

\paragraph{Independence:} Two random variables $X,Y$ on a probability space $(\Omega,\mathcal{F},P)$ are independent if \[ P(A \cap B)=P(A)P(B) \] for every $A\in \sigma(X), B\in \sigma(Y).$

\paragraph{Expectation:} Let $X$ be a random variable on a probability space $(\Omega,\mathcal{F},P)$ and $g:\mathbb{R} \to \mathbb{R} $. We define the expectation of $g(X)$  
\[ E(g(X))=\int_\Omega g(X) dP,\] if this Lebesgue integral exists.

\paragraph{Conditional expectation:} Let $X$ be a random variable on a probability space $(\Omega,\mathcal{F},P)$ and let $\mathcal{H}$ be a sub $\sigma$-algebra of $\mathcal{F}$. A conditional expectation $E(X|\mathcal{H})$ of $X$ with respect to $\mathcal{H}$ is a random variable on $(\Omega,\mathcal{H})$ defined through the following property
\[ \int_H E(X|\mathcal{H}) dP = \int_H X dP \] for every $H\in \mathcal{H}.$ It is unique up to a set of measure $0$. 
Let $Y$ be another random variable on this probability space. The conditional expectation $E(X|Y)$ of $X$ conditional on $Y$ is defined by

\[ E(X|Y)=E(X|\sigma(Y)).\]



\paragraph{Almost sure convergence:} A sequence of random variables $(X_n)_{n=1}^\infty$ on a probability space $(\Omega,\mathcal{F},P)$ convergences almost surely to a random variable $X$ on the same space if $$P( \lim_{n\to \infty} X_n \ne X)=0.$$

\paragraph{Convergence in probability:} A sequence of random variables $(X_n)_{n=1}^\infty$ on a probability space $(\Omega,\mathcal{F},P)$ convergences in probability to a random variable $X$ on the same space if $$\forall \epsilon>0: \lim_{n\to \infty} P\{ |X_n-X|>\epsilon \}=0.$$

\paragraph{Convergence in mean square:} We say that a sequence of random variables $(X_n)_{n=1}^\infty$ on a probability space $(\Omega,\mathcal{F},P)$ convergences in mean square to a random variable $X$ on the same space if 
$$ \lim_{n\to \infty}E\left( (X_n-X)^2 \right) =0.$$
 



\begin{exercise}
Give the smallest $\sigma$-algebra ${\cal F}$ on the set $\Omega = \{1,2,3,4,5,6\}$. 
\end{exercise}

\begin{exercise}
Give the largest $\sigma$-algebra ${\cal F}$ on the set $\Omega = \{1,2,3,4,5,6\}$. It can be used to describe all events of throwing a regular dice.
\end{exercise}

\begin{exercise}
Give the $\sigma$-algebra ${\cal F}$ on the set $\Omega = \{1,2,3,4,5,6\}$ generated by $\{ \{1,3,5\} \}.$ What kind of events in throwing a regular dice can it describe? 
\end{exercise}

\begin{exercise}
Show that a $\sigma$-algebra is closed under countable intersections
\end{exercise}

\begin{exercise}
Give the smallest $\sigma$-algebra ${\cal F}$ on the set $\mathbb{R}$. 
\end{exercise}

\begin{exercise}
Give the smallest $\sigma$-algebra ${\cal F}$ on the set $\mathbb{R}$ that can be used to describe wether a real number is (strictly) positive or not.
\end{exercise}

\begin{exercise}
How does the smallest $\sigma$-algebra ${\cal F}$ on the set $\mathbb{R}$ that can be used to check whether a real number lies in an open interval $]a,b[$ for all real numbers $a\le b$ look like? Show it must contain all intervals $]a,b[$, $[a,\infty[$, $[-\infty,a]$, $[a,b]$, $]a,b]$, $[a,b[$, $\mathbb{N}$, $\mathbb{Z}$, $\mathbb{Q}$. Show that ${\cal F}$ is the Borel sigma algebra on $\mathbb{R}$. 
\end{exercise}

\begin{exercise}
Describe a probability space $(\Omega, {\cal F}, P)$  representing throws of a fair regular dice.
\end{exercise}

\begin{exercise}
Give an example of a probability space $(\mathbb{N}, {\cal P}(\mathbb{N}), P)$  such that $P(\{n\})>0$ for all $n \in \mathbb{N}$. Note that it suffices to define $P$ on all singletons, since every subset $S$ of $\mathbb{N}$ is a countable union of singletons. So its probability measure is implicitly defined: it equals the sum of the measures of all singletons in $S$ and this sum exists as it is a sum of positive integers bounded by $P(\mathbb{N})=1.$  Also note that it is imposible to take  $P(\{n\})$ equal for all $n$. So drawing a random uniform natural number is impossible. In order to draw random natural numbers you hence need to specify a probability space.
\end{exercise}

\begin{exercise}
It is challenging to find a measure space $(\mathbb{R}, {\cal F}, \mu)$ with the following natural properties: ${\cal F}$ contains all intervals, $\mu([0,1])=1$, $\mu(A)=\mu(A+x)$ for all $x \in \mathbb{R}$ and $A \in {\cal F}$ (translation invariant), $A \in {\cal F}$ for all $A \subset B$ for all $B \in {\cal F}$ with $\mu(B)=0$ (completeness). You may try to define such measure space but don't get frustrated: it is a deep result in measure theory. The Lebesgue measure space $(\mathbb{R}, {\cal F}, \mu)$ is the unique space satisfying these properties. Its sigma algebra contains all intervals $(a,b)$ for $b>a$ and their measure equals $\mu((a,b))=b-a$. It is not evident to construct it and prove it satisfies the properties of $\sigma$-algebra and measure. It does contain the Borel $\sigma$-algebra ${\cal B}(\mathbb{R})$. In fact it is a bit larger than ${\cal B}(\mathbb{R})$. The Borel $\sigma$-algebra  ${\cal B}(\mathbb{R})$ contains sets of Lebesgue measure $0$ that have subsets not in ${\cal B}(\mathbb{R})$. The Lebesgue $\sigma$-algebra is then the completion of the Borel $\sigma$-algebra in the sense that it is the smallest $\sigma$-algebra containing the Borel $\sigma$-algebra and all subsets of borel sets with measure $0$. 
For more informaton see \href{https://en.wikipedia.org/wiki/Lebesgue_measure}{Lebesgue measure}.
\end{exercise}

\begin{exercise}
A famous paradox in probability theory is Bertrand's paradox: Consider an equilateral triangle inscribed in a circle. When a chord of the circle is chosen at random, what's the probability that the chord is longer than a side of the triangle? You can compute the probability in three different ways yielding three different results. That's the paradox. See \href{https://brilliant.org/wiki/paradoxes-in-probability}{Bertrand paradox} 
The problem is that it is not well defined what is meant with chosing a random chord of the circle. In each of the three solutions a different probability space is considered: describe this space in each of the cases. 
\end{exercise}

\begin{exercise}
Give an explicit description of a standard normal random variable $X$ and its probability space $(\Omega, {\cal F}, P)$ 
\end{exercise}

\begin{exercise}
Let  $(\Omega, {\cal F}, P)$ be a probability space on which two sub $\sigma$-algebras ${\cal G}_1 \subseteq {\cal G}_2 \subseteq {\cal F}$ are defined. For a random variable $X$ on such a space, prove the tower law that states that if 
$E[X]$ is defined, then

$$E[E[X|{\cal G}_2]|{\cal G}_1]=E[X|{\cal G}_1]  \quad \text{(a.s.)}$$  We often use the tower law for a stochastic process with a filtration $({\cal F}_t)$ in the sense that $E[g(X_T)] = E[E[g(X_T)|{\cal F}_t]$

In the definition of conditional expectation we condition on a random variable (i.e. the $\sigma$-algebra it generates) $E[X|Y]$. In particular we do not define such thing like $E[X|Y=y].$ This is because the event ${Y=y}$ might have $0$ probability and in such cases we cannot define it in a consistent manner as explained by the \href{https://en.wikipedia.org/wiki/Borel-Kolmogorov_paradox#cite_note-Jaynes-1}{Borel-Koolmogorov paradox}
\end{exercise}


\begin{exercise}
Show that almost sure convergence implies convergence in probability and convergence in mean square implies convergence in probability. See also \href{https://en.wikipedia.org/wiki/Convergence_of_random_variables}{Convergence of random variables}. 
\end{exercise}

\subsection{Stochastic Processes}

\paragraph{Stochastic process:} A stochastic process on a probability space $(\Omega,\mathcal{F},P)$ is a collection of random variables $(X_t)_{t\in \mathbb{R}^+}$ on that space. A stochastic process can also be seen as a function $X:\mathbb{R} \times \Omega \to \mathbb{R}$ where each $X_t:\Omega \to \mathbb{R}:\omega \to X(t,\omega)$ is a random variable on the probability space $(\Omega,\mathcal{F},P).$ The functions $X(-,\omega):\mathbb{R}^+ \to \mathbb{R}:t\to X(\omega,t)$ for $\omega \in \Omega$ are called the sample paths of the process.

\paragraph{Filtered probability space:} A filtered probability space $(\Omega,\mathcal{F},P,(\mathcal{F}_t)_{t\in \mathbb{R}^+})$ is a probability space $(\Omega,\mathcal{F},P)$ together with a filtration $(\mathcal{F}_t)_{t\in \mathbb{R}^+},$ i.e. an indexed set of $\sigma$-algebras such that 
$$ \mathcal{F}_t \subset \mathcal{F}_s \textup{ for all } t\leq s.$$
The filtration represents the information available up to time $t.$

\paragraph{Adapted process:} Given a filtered probability space $(\Omega,\mathcal{F},P,(\mathcal{F}_t)_{t\in \mathbb{R}^+})$ a stochastic process $X_t$ on $(\Omega,\mathcal{F},P)$ is adapted to the filtration if 
$$ \forall t \in \mathbb{R}^+, \forall A \in \mathcal{B}(\mathbb{R}):  X_t^{-1}(A)\in \mathcal{F}_t.$$ Intuitively an adapted process is one that cannot see into the future. 

\paragraph{Predictable process:} A stochastic process $X$ on a filtered probability space $(\Omega,\mathcal{F},P,(\mathcal{F}_t)_{t\in \mathbb{R}^+})$ is a predictable process if it is a measurable function from $( \mathbb{R} \times \Omega, \Sigma$) to $(\mathbb{R},\mathcal{B}(\mathbb{R})),$ where $\Sigma$ denotes the $\sigma$-algebra generated by all left continuous adapted processes on the filtered probability space. Intuitively a predictable process is one that is predictable from prior times. Note that adapted left continous processes clearly satisfy the definition and are predictable in this intuitive sense as its value at a given time $t$ is (a.s.) equal to the limit of its values at prior times. 

\paragraph{Partition:} a sequence $x=(x_0,\dots,x_n)$ of real numbers with $a=x_0<x_1<\hdots<x_n=b$ is called a partition of the interval $[a,b].$ 
 

\paragraph{Total variation:} The total variation $V_a^b(f)$ of a function $f:[a,b]\to \mathbb{R}$ is defined as

$$ \sup_{x \in \mathcal{X}} \sum_{i=1}^{N_x} |f(x_i)-f(x_{i-1})|,$$

where $$\mathcal{X}=\{x=(x_1,\dots,x_{N_x})|x \textup{ is a partition of } [a,b]\}.$$

\paragraph{Finite variation:} A function $f:\mathbb{R}^+ \to \mathbb{R}$ has bounded (or finite) variation if its total variation over every finite interval is finite. Otherwise it has unbounded variation. A random process $X$ has finite variation if (almost) all its sample paths have finite variation. 

\paragraph{Norm (or mesh) of a partition:} The norm $|x|$ of a partition $x=(x_0,x_2,\hdots ,x_n)$ is the maximum of its increments
$$ |x|=\max_{i=1}^n (x_i-x_{i-1}).$$



\paragraph{Quadratic variation:} The quadratic variation $[X]_t$ of a stochastic process $X$ on a probability space $(\Omega,\mathcal{F},P)$ is defined as 

$$ [X]_t=\lim_{|\pi|\to 0} \sum_{i=1}^n (X(t_i)-X(t_{i-1}))^2,$$

where $\pi=(t_0,t_1,\hdots,t_n)$ runs over partitions of $[0,t]$ and the limit is taken with respect to convergence in probability.

\paragraph{Quadratic covariation:} The quadratic covariation $[X,Y]_t$ of  2 stochastic processes $X$ and $Y$ on a probability space $(\Omega,\mathcal{F},P)$ is defined as 

$$ [X,Y]_t=\lim_{|\pi|\to 0} \sum_{i=1}^n (X(t_i)-X(t_{i-1}))(Y(t_i)-Y(t_{i-1})),$$

where $\pi=(t_0,t_1,\hdots,t_n)$ runs over partitions of $[0,t]$ and the limit is taken with respect to convergence in probability.

\paragraph{C\`{a}dl\`{a}g and c\`{a}gl\`{a}d:} a c\`{a}dl\`{a}g, resp. c\`{a}gl\`{a}d, stochastic process $X$ is a process whose sample paths are right continuous with left limits (continue \`{a} droite, limite \`{a} gauche), resp. left continuous with right limits (continue \`{a} gauche, limite a droite).

\paragraph{Martingale:}  Given a filtered probability space $(\Omega,\mathcal{F},P,(\mathcal{F}_t)_{t\in \mathbb{R}^+})$ an adapted stochastic process $X_t$ is a martingale if its sample paths are almost surely c\`{a}dl\`{a}g and for each $s$ and $t>s$ we have 
\[ E(|X(t)|)<\infty \]  
and
\[ E(X(t)|\mathcal{F}_s)=X(s).\]

\paragraph{Stopping Time:} Given a filtered probability space $(\Omega,\mathcal{F},P,(\mathcal{F}_t)_{t\in \mathbb{R}^+})$, a random variable $\tau:\Omega \to \mathbb{R}^+$ is a stopping time if for all $t \in  \mathbb{R}^+$
$$ \{ \omega \in \Omega | \tau(\omega) <t \} \in \mathcal{F}_t.$$

\paragraph{Local Martingale:}   Given a filtered probability space $(\Omega,\mathcal{F},P,(\mathcal{F}_t)_{t\in \mathbb{R}^+})$. A stochastic process $X_t$ adapted to this filtration is a local martingale if there exists a sequence of stopping times $\tau_k$ such that
\begin{itemize}
\item the $\tau_k$ are almost surely increasing:$P(\tau_k< \tau_{k+1})=1$
\item the $\tau_k$ diverge almost surely: $P(\lim_{k\to \infty} \tau_k= \infty)=1$
\item the stopped processes $X_{\min(\tau_k,t)}$ are martingales for all $k$
\end{itemize}

\paragraph{Locally bounded:}  Given a filtered probability space $(\Omega,\mathcal{F},P,(\mathcal{F}_t)_{t\in \mathbb{R}^+})$. A stochastic process $X_t$ is a locally bounded process if there exists a sequence of stopping times $\tau_k$ such that
\begin{itemize}
\item the $\tau_k$ are almost surely increasing:$P(\tau_k< \tau_{k+1})=1$
\item the $\tau_k$ diverge almost surely: $P(\lim_{k\to \infty} \tau_k= \infty)=1$
\item the stopped processes $X_{\min(\tau_k,t)}$ is bounded for all $k$
\end{itemize}

\paragraph{Semimartingale:} Given a filtered probability space $(\Omega,\mathcal{F},P,(\mathcal{F}_t)_{t\in \mathbb{R}^+})$. A semi-martingale is a stochastic process on this space that is the sum of a martingale and a c\`{a}dl\`{a}g adapted process with finite variation.


\begin{exercise}
Find a function with unbounded variation.
\end{exercise}

\begin{exercise}
Find a continuous function with unbounded variation but bounded quadratic variation.
\end{exercise}

\begin{exercise}
Prove that a continuously differentiable function has $0$ quadratic variation.
\end{exercise}
 

 

\section{Brownian Motion}

A Brownian motion on a probability space $(\Omega, \mathcal{F},P)$ is defined through its properties. It is a stochastic process $B(t)$ on the space such that 

\begin{itemize}
\item $B(0)=0$
\item $B(t)-B(s)$ has a normal $N(0,t-s)$ distribution for $s\leq t$
\item its increments over disjoint time intervals are independent
\item its paths are almost surely continuous
\end{itemize}

\begin{figure}[ht]%
\centering
%trim option's parameter order: left bottom right top
\includegraphics[trim =0mm 0mm 0mm 0mm, clip, width=\textwidth]{FIGURES/brownian_5.jpg}%
\caption{5 sample paths of Brownian motion}%
\label{f:brownian_5}%
\end{figure}

\begin{figure}[ht]%
\centering
%trim option's parameter order: left bottom right top
\includegraphics[trim =0mm 0mm 0mm 0mm, clip, width=\textwidth]{FIGURES/brownian_100.jpg}%
\caption{100 sample paths of Brownian motion}%
\label{f:brownian_100}%
\end{figure}

Note that it requires some work to prove that Brownian motion exists. Moreover one can show the following properties

\begin{itemize}

\item Brownian motion is almost surely nowhere differentiable
\item Brownian motion has amost surely unbounded variation
\item Brownian motion has quadratic variation $[B]_t=t.$

\end{itemize}

The construction of Brownian motion is based on the following property of normal distributions. Given

\begin{itemize}
\item $X_1  \sim N(0,\sigma_1^2)$
\item $X_2 \sim N(0,\sigma_2^2)$
\item $X_1, X_2$ are independent
\end{itemize}

then

$$ X=X_1+X_2 = N(0,\sigma_1^2 + \sigma_2^2).$$

We can use this idea to construct Brownian motion on a discrete time grid $[0,\Delta,2 \Delta, ..., N \Delta]$. Choose $N$ independent random variables $X_i \sim N(0,\Delta)$. Set 

$$ B(0)=0 $$
$$ B(i \Delta)=B((i-1) \Delta) + X_i.$$


%\section{Girsanov's Theorem}
%
%\paragraph{Girsanov's theorem:} Let $B(t)$ be Brownian motion on a probability space $(\Omega,\mathcal{F},P).$ Let $X(t)$ be a stochastic process on this space adapted to the natural filtration $\mathcal{F}_t$ of $B(t).$ Define the Doleans-Dade exponential of $X(t)$ by 
%$$\mathcal{E}(X)_t= e^{X(t)-\frac{1}{2}[X]_t}.$$
%If $\mathcal{E}(X)_t$ is a strictly positive martingale, a probability measure $Q$ on $(\Omega,\mathcal{F})$ can be defined such that its Radon-Nykodim derivative equals
%$$ \frac{dQ}{dP}|\mathcal{F}_t = \mathcal{E}(X)_t$$ and 
%$Q$ is equivalent to $P$ on each $\mathcal{F}_t$. The opposite also holds. Furthermore, if $Y^P_t$ is a $P$-local martingale on $(\Omega,\mathcal{F},P,\mathcal{F}_t)$ , then 
%$$ Y^Q_t=Y^P_t-[X,Y^P]_t$$ 
%is a $Q$-local martingale on $(\Omega,\mathcal{F},Q,\mathcal{F}_t).$
%In the special case that $Y^P_t$ is a $P$-Brownian motion and $X_t$ is continuous, $Y^Q_t$  is 
%$Q$-Brownian motion.
%
%This theorem is often applied to the case when 
%
%$$ X_t=\int_0^t \alpha(s) dB(s).$$
%
%Here $\alpha$ does not need to be a deterministic function of time (it can be a stochastic process as long as the integral can be defined). In the special case that $(B,Y^P_t$ is joint $P$-Brownian motion with correlation $\rho(t)$ and $\alpha(s)$ is deterministic then we have
%
%$$ dY^Q(t)+\alpha(t)\rho(t)dt=dY^P(t).$$
%
%In other words, under $Q$ the process $Y^P(t)$ is Brownian motion plus drift, where the drift coefficient is given by $\alpha(t) \rho(t).$
%
%
%A multivariate extension of Girsanov's theorem leads to the following result that can be used as a toolkit for changing numeraires in case of Brownian motions.
%
%\paragraph{Change of numeraire toolkit:}
%Let $X(t)=(X_i(t))$ be an $n$-dimensional process satisfying 
%$$ \frac{dX_i(t)}{X_i(t)}=\mu_i^S(t)dt + \sigma_i(t) C_i dB^S(t), $$
%where the volatility coefficients $\sigma_i(t)$ are deterministic functions of time, the $C_i$ are $1\times n$ vectors and $B^S(t)$ is $n \times 1$ standard Brownian motion under the measure associated to a numeraire $S$. Here the matrix $C$ with the $C_i$ as rows is used to induce a correlation matrix $\rho=CC'.$ 
%Under another numeraire $U$ we then have (by Girsanvov's theorem)
%$$ \frac{dX_i(t)}{X_i(t)}=\mu_i^U(t)dt + \sigma_i(t) C_i dB^U(t)$$
%for a standard Brownian motion $B^U(t)$ under the measure associated to $U$. 
%Then 
%
%$$ d\mu_i^U(t)=d\mu_i^S(t)-d\left\langle \ln X_i(t),\ln (S/U) \right\rangle.$$
%
%Note that the drift coefficients $\mu_i^S(t)$ and $mu_i^U(t)$ may be stochastic.
% 



 

\section{It\^{o} integral with respect to Brownian motion}

Many processes in finance are defined by integrating Brownian motion. More precisely we need integrals like 

$$ \int_0^t f(s,B(s)) d s $$

and

$$ \int_0^t f(s,B(s)) d B(s) $$

for a smooth function $f$. The first integral can be defined as a pathwise Riemann-Stieltjes integral since Brownian motion is (a.s.) continuous. In general, the second integral cannot be defined as a pathwise integral since the integrator $B(s)$ is not of bounded variation. However it is defined as the limit in probability of sums of random variables over a partition

$$ \int_0^t f(s,B(s)) dB(s) = \textup{lim}_{|\Pi|\to 0} Q_{\Pi}(t) $$ 
where \[ Q_{\Pi}(t)=\sum_{i=1}^N f(t_{i-1},B(t_{i-1})) (B(t_i)-B(t_{i-1})) ,\]

$\Pi=\{ t_0=0,t_1,...,t_N=t\}$ is a partition of $[0,t]$ and $|\Pi|=\textup{max}_i |t_i-t_{i-1}|$ is a norm on the partition. It is a result of stochastic calculus theory that this limit exists and we call it the It\^{o} integral. Note that whenever $f$ satisfies

$$ \int_0^t E(f(s,B(s))^2) ds< \infty $$

the limit can be taken in the mean square sense (see \cite{Mikosch}). Ito first defined the integral as a pathwise integral requiring the same condition and fast enough convergence of the norm of the partition (see \cite{ItoIntegral}). De La Vega showed that the limit of the partial sums does not need to converge pathwise in case of slower convergence speed (see \cite{DeLaVega}).  

The fact that we evaluate $f$ in the left hand point of the partition intervals is crucial for the It\^{o} integral and yields a martingale (as the increment of Brownian motion over a time interval does not depend on its value at the start of the interval). 

As an example consider $$\int_0^t B(s) dB(s).$$ Taking a uniformely spaced partition $\Pi=\{ t_0=0,t_1=t/N,t_2=2t/N,...,t_N=t\}$ and denoting $\Delta B_i=B(t_i)-B(t_{i-1})$ we have 

\begin{eqnarray*}
S_N(t) & := & \sum_{i=1}^N B(t_{i-1}) (B(t_i)-B(t_{i-1})) \\
& = &  \sum_{i=1}^N \sum_{k=1}^{i-1} \Delta B_k \Delta B_i \\
&=& \frac{1}{2} \sum_{i=1}^N \sum_{k<>i} \Delta B_k \Delta B_i \\
&=& \frac{1}{2} \left(\sum_{i=1}^N \sum_{k=1}^N \Delta B_k \Delta B_i - \sum_{i=1}^N (\Delta B_i)^2 \right) \\
&=& \frac{1}{2} \left( (\sum_{i=1}^N \Delta B_i)^2 - \sum_{i=1}^N (\Delta B_i)^2 \right) \\
&=& \frac{1}{2} \left( B(t)^2 - \frac{t}{N}\sum_{i=1}^N \frac{N (\Delta B_i)^2}{t} \right) 
\end{eqnarray*}

Now the sum in the right hand term is a sum of independent squares of standard normal random variables. Due to the law of large numbers its average converges in probability to the expectation of one such square (that equals $1$). Hence

$$\int_0^t B(s) dB(s) = \frac{1}{2} \left( B(t)^2 - t \right)$$ 

or 

$$ B(t)^2  = 2 \int_0^t B(s) dB(s) +t.$$

Our computation also shows that the quadratic variation of Brownian motion, given it exists, equals $[B(t)]=t$ (in many text books this is loosely written as $(dB)^2=dt$). The resulting equation is often also written as 

\[ d\left(B(t)^2 \right) = 2B(t)dB(t)+d[B(t)].\]


Note that for deterministic differentiable functions $f(t)$ (with $f(0)=0$) the fundamental theorem of calculus says

$$ f(t)^2 = 2 \int_0^t f(s) df(s).$$

Hence the second term on the right-hand side is typical for stochastic calculus. We can easily check it must be there by taking expectations. Indeed,

\begin{eqnarray*}
E(B(t)^2) &   = & 2 E(\int_0^t B(s) dB(s)) +t \\
&=& 2 \int_0^t E(B(s)) E(dB(s)) +t \\
&=&t
\end{eqnarray*}

Here we used the fact that $B(s)$ and $dB(s)$ are independent.

This equation must indeed hold since $B(t) \sim N(0,t)$. Thus the second term is indeed necessary.

\begin{exercise} Show that the quadratic variation of Brownian motion exists and equals $[B(t)]=t.$ You need to consider arbitrary partitions, instead of the equally spaced partitions we worked with above. Hint: show that the partition sums converge in mean square sense. 
\end{exercise}

\begin{exercise}
Implement code to generate sample paths of Brownian motion at a discrete time grid
\end{exercise}

\begin{exercise}
Show that Brownian motion has almost surely unbounded variation. Hint: This is quite hard to prove directly and therefore often proved by contradiction. Hence assume Brownian motion has bounded variation on some interval $[a,b]$ (with $b>a$) and a set $A \subset \Omega$ with $P(A)>0$. Then show it must have $0$ quadratic variation on $[a,b]$ on a subset $B \subset \Omega$ with $P(B)>0$, in contradiction with its known quadratic variation $b-a>0$.
\end{exercise}

\begin{exercise}
Let $\hat{B}(t) := \sup_{0 \le s \le t} B(s)$ for a Brownian motion $B(t)$. Prove that $P(\hat{B}(t) \ge b)= 2P(B(t) \ge b)$ for $b \ge 0.$ This is a nice property of Brownian motion that can be exploited in pricing barrier options. It uses the reflection principle, see for instance \href{https://almostsuremath.com/2023/04/18/the-maximum-of-brownian-motion-and-the-reflection-principle}{maximum of Brownian motion} 
\end{exercise}

\begin{exercise} Suppose that we replace $Q_{\Pi}(t)$ by 

\[ Q^a_{\Pi}(t)=\sum_{i=1}^N \left( (1-a)f(t_{i-1},B(t_{i-1}))+af(t_{i},B(t_{i})) \right) (B(t_i)-B(t_{i-1})) \]

in the definition of the stochastic integral. Show that then  

$$\int_0^t B(s) dB(s) = \frac{1}{2} \left( B(t)^2 -t \right)+at.$$ What happens when $a=\frac{1}{2} ?$ 

\end{exercise}

\begin{exercise} Two-dimensional Brownian motion is defined similarly as 1-dimensional Brownian motion. it is a 2-dimensional stochastic process $(B_1(t),B_2(t))$ on a probability space such that 
\begin{itemize}
\item $B_1(0)=B_2(0)=0$
\item $(B_1(t)-B_1(s),B_2(t)-B_2(s))$ has a 2-dimensional normal distribution $N(0,(t-s)C)$ for $s\leq t$ where $C$ denotes the correlation matrix
 $$C=
 \begin{pmatrix}
  1 & \rho \\
  \rho & 1
 \end{pmatrix}
$$
\item its increments over disjoint time intervals are independent
\item its paths are almost surely continuous
\end{itemize}
Show that the quadratic covariation $[B_1,B_2](t)=\rho t$ (in many text books this is written as $dB_1dB_2=\rho dt$) and show that 
\[ d(B_1(t)B_2(t))=B_1(t)dB_2(t)+B_2(t)dB_1(t)+\rho dt.\]

\end{exercise}

\begin{figure}[ht]%
\centering
%trim option's parameter order: left bottom right top
\includegraphics[trim =0mm 0mm 0mm 0mm, clip, width=\textwidth]{FIGURES/brownianIntegral_5.jpg}%
\caption{Integrating Brownian motion}%
\label{f:brownianIntegral_5}%
\end{figure}

\paragraph{It\^{o} Isometry:} The It\^{o} integral satisfies the isometry property

$$ \EV{\left(\int_0^t f(s,B(s)) dB(s)\right)^2}=\EV{\int_0^t f(s,B(s))^2 ds}.$$

In particular, for a deterministic function $f(s)$ we have

$$ \EV{(\int_0^t f(s) dB(s))^2}=\int_0^t f(s)^2 ds.$$

It follows that 

$$ \int_0^t f(s) dB(s) \sim N(0,\int_0^t f(s)^2 ds).$$



\section{It\^{o}'s Lemma}

\paragraph{Lemma:} \label{par:lemma} Let $f:\mathbb{R} \to \mathbb{R} $ be a continuous function,then

$$ \sum_{i=1}^N f(B(t_{i-1})) (B(t_i)-B(t_{i-1}))^2 $$ converges in probability to 
$$\int_0^T f(B(t)) dt$$
when $N \to \infty.$ Here $t_i=iT/N.$

Proof: First assume $|f|$ is bounded by some constant C. It suffices to prove that 

$$ I_n= \sum_{i=1}^N f(B(t_{i-1})) \left( (B(t_i)-B(t_{i-1}))^2 - (t_i-t_{i-1}) \right) $$

converges in probability to 0. To do so we show that $\EV{I_n^2}$ converges to $0$. Denote $f(B(t_i))$ by $f_i$, $B(t_i)-B(t_{i-1})$ by $\Delta B_i$, $t_i-t_{i-1}$ by $\Delta t_i.$ Then 


\begin{eqnarray*}
\EV{I_n^2} & = & \sum_{i=1}^N \sum_{j=1}^N \EV{f_{i-1} f_{j-1} ((\Delta B_i)^2 -\Delta t_i)((\Delta B_j)^2 - \Delta t_j)} \\
& = & \sum_{i=1}^N \EV{f_{i-1}^2} \EV{((\Delta B_i)^2 -\Delta t_i)^2)} \\
& \leq & C^2 \sum_{i=1}^N (\Delta t_i)^2 \EV{(B_1^2-1)^2} \\
& \leq & C^2 \EV{(B_1^2-1)^2} |\Pi| T 
\end{eqnarray*}

Note here that the expectation over the cross  terms equals zero since increments of Brownian motion over a time interval are independent from the value at the start of the interval. 
This expectation clearly converges to $0$ as $N \to \infty.$ In the general case when $f$ is not necessarily bounded, we may replace $f$ by $f \psi$ where $\psi(x)$ is a smooth function that equals $1$ if $|x|\leq M$ and $0$ if $|x|\geq M+1$ for some constant $M$. The lemma then holds on the subset of paths for which $|B(t)|\leq M$ on $[0,T].$ As $M \to \infty$ the probability of this subset goes to $1.$ Hence in the unbounded case convergence in probability holds. $\qed$

\begin{exercise} Show that the sums $$I_n= \sum_{i=1}^N f(B(t_{i-1})) \left( (B(t_i)-B(t_{i-1}))^2 - (t_i-t_{i-1}) \right)$$ converge in mean square sense when 
$$ \int_0^T E(f(t)^2) dt < \infty.$$
\end{exercise}

\paragraph{It\^{o}'s Lemma:} 
Let $f:\mathbb{R} \times \mathbb{R^+} \to \mathbb{R}$ be a function in two variables $x$ and $t$ with continuous first and second order partial derivatives. Then for all $T>0$

$$ f(B(T),T)-f(0,0)= \int_0^T f_t(B(t),t) dt + \int_0^T f_x(B(t),t) dB(t) + \frac{1}{2} \int_0^T f_{xx}(B(t),t) dt.$$

Proof:

We first prove the lemma in case $f$ also has a continuous third order partial derivative in $x$.
Using the same  partition and notation as before ($f_{x,i}=f_x(B(t_i),t_i)$, ...) and using Taylor's expansion we have

\begin{eqnarray} 
f(B(T),T)-f(0,0) & = &\sum_{i=1}^N f_i - f_{i-1}  \\ 
& = & \sum_{i=1}^N f_{t,i} \Delta t_i \label{eq:Ito1} \\
& + & \sum_{i=1}^N f_{x,i}  \Delta B_i \label{eq:Ito2} \\
& + & \frac{1}{2} \sum_{i=1}^N f_{xx,i} \Delta B_i^2  \label{eq:Ito3} \\
& + & \sum_{i=1}^N O(\Delta t_i \Delta B_i) \label{eq:Ito4}\\
& + & \sum_{i=1}^N O(\Delta t_i^2) \label{eq:Ito5}\\
& + & \sum_{i=1}^N O(\Delta B_i^3) \label{eq:Ito6}
\end{eqnarray}

Due to the assumption on the existence and continuity of the partial derivatives and Taylor's theorem, we have,

\begin{eqnarray*}
|\sum_{i=1}^N O(\Delta t_i \Delta B_i)| & \leq & K T \sup_i |\Delta B_i| \\
|\sum_{i=1}^N O(\Delta t_i^2)| & \leq & K T \sup_i |\Delta t_i| \\
|\sum_{i=1}^N O(\Delta B_i^3)| & \leq & K \sup_i |\Delta B_i| \sum_{i=1}^N \Delta B_i^2
\end{eqnarray*}

for some $K>0$ that does not depend on $N$.
The terms \ref{eq:Ito4},\ref{eq:Ito5} and \ref{eq:Ito6} then converge to 0 in probability due to our previous lemma and the continuity of $B(t).$

The first term \ref{eq:Ito1} converges to the pathwise Riemann integral and the second term \ref{eq:Ito2} to the It\^{o} integral. For the third term \ref{eq:Ito3} we may use our previous lemma yielding the required result. 

We now turn to the general case where we only require $f$ to be a $C^2$ function. Due to the Stone-Weierstrass theorem we can find a series of polynomials $g_m$ that converge, as well as their first and second order partial derivatives, to $f$ and its first and second order partial derivatives, uniformely on the compact set $B_n=\{(x,t): |x|\leq n, 0\leq t \leq T\}.$ It then follows from the properties of the It\^{o} integral that the integrals in the It\^{o} formula for $g_m$ converge in probability to the integrals in the It\^{o} formula for $f$ on this set $B_n$. Since $P(B_n)\to 1$ as $N\to \infty$, it follows that 
they also converge in probability on the whole domain. The result now follows from the first part of the proof (since polynomials have continuous third order partial derivatives). $\qed$

Note that It\^{o}'s lemma is often written in differential form (but strictly spoken this is non-sensical as $B(t)$ is nowhere differentiable)

$$ df(B(t),t)=f_t(B_t,t) dt + f_x(B_t,t) dB(t) + \frac{1}{2} f_{xx}(B(t),t) dt.$$



\section {It\^{o} processes}

Here we generalise the It\^{o} integral to integrals with respect to slightly more general processes. We say an It\^{o} process on a  filtered probability space $(\Omega,\mathcal{F},P,(\mathcal{F}_t))$ is any adapted stochastic process satisfying:

\begin{equation}\label{eq:ItoProcess}
X(t)=X(0)+\int_0^t \mu(s) ds + \int_0^t \sigma(s) dB(s),
\end{equation}

where $B(t)$ is a Brownian motion on the space and $\mu(t)$ and $\sigma(t)$ are predictable processes such that $\mu(t)$ is (Lebesgue) integrable and $\sigma(t)$ is $B$-integrable. Note that the definition allows $\mu$ and $\sigma$ to be stochastic processes. They may even depend on $X(t)$ itself, such that the definition is implicit. We can also write it as a SDE:

$$ dX(t)=\mu(t)dt+\sigma(t)dB(t).$$ 

It\^{o}'s lemma can then be extended to It\^{o} processes.

\paragraph{It\^{o}'s lemma for It\^{o} processes:} Let $f:\mathbb{R} \times \mathbb{R^+} \to \mathbb{R}$ be a function in two variables $x$ and $t$ with continuous first and second order partial derivatives. Let $X$ be an It\^{o} process on a filtered probability space $(\Omega,\mathcal{F},P,(\mathcal{F}_t))$  defined as in equation \ref{eq:ItoProcess}.

Then $f(X(t),t)$ is again an It\^{o} process and for all $T>0$

$$ f(X(T),T)-f(0,0)= \int_0^T f_t(X(t),t) dt + \int_0^T f_x(X(t),t) dX(t) + \frac{1}{2} \int_0^T f_{xx}(X(t),t) \sigma^2(t) dt.$$
 
The proof is essentially the same as before noting that for increments $(dX(t))^2=\sigma^2(t)(dB(t))^2+O(dtdB(t)).$ 
See \cite{ItoLemma} for a rigorous proof.

\begin{exercise}Assume $x(t)$ is a stochastic process that satisfies
\[ dx(t)=-\kappa x(t)dt+\sigma dB(t).\] Show that \[x(t)=x(0)e^{-\kappa t} + \sigma \int_0^t e^{-\kappa(t-u)} dB(u).\]
(Hint: use It\^{o}'s lemma with $f(x,t)=xe^{\kappa t}.)$ Next use It\^{o}'s isometry to show that \[x(t) \sim N(x(0)e^{-\kappa t},\frac{\sigma^2}{2\kappa}(1-e^{-2\kappa t})).\]
\end{exercise}

\section{Semimartingales}

The whole theory of Ito stochastic integration extends to a very wide class of processes. This theory is developed in e.g.  \cite{JacodShiryaev} where one defines

$$ \int_0^t H(s) d X(s) $$

for a semimartingale $X$ and a locally bounded predictable process $H$. The Ito integral extends to functions of semimartingales in higher dimensions. Note that semimartingales are not restricted to continuous functions.


\chapter{Derivative Valuation}

An excellent introduction to the theory of derivative pricing is \cite{BaxterRennie}. The fundamental asset pricing theorems were first stated by Harrison and Pliska \cite{HarrisonPliska}. 

\section{Fundamental Asset Pricing Theorems}

The use of stochastic processes in valuating instruments is based on the so-called fundamental asset pricing theorem. It states that we can compute the fair value of a derivative on an asset by computing its discounted expected payoff in the so-called risk neutral world, meaning that we need to change the probabilities of the stochastic payoff in a specific way. In such a risk neutral world, also the current fair value of the asset itself must equal the expectation of its discounted value at any future time. Often the model for the asset is specified in the risk neutral world and calibrated such that this holds. In the risk neutral world, stock prices will have an expected growth rate equal to the risk free spot rate. 

Note that the payoff of more complex derivatives (such as lookback options) may depend on the value of the asset at different future times. Hence we need a stochastic process to model the asset, not just a random variable.

\paragraph{Asset:} an asset is a positive adapted stochastic process $A(t)$ on a filtered probability space $(\Omega,\mathcal{F},P,(\mathcal{F})_t).$ The process describes the stochastic value of a non-dividend paying tradable asset. Examples are the bank account or a zero coupon bond.

\paragraph{Market model:} a market model $(N,A_i)$ on a filtered probability space $(\Omega,\mathcal{F},P,(\mathcal{F})_t)$ consists of a number of tradable assets. The first asset is called the numeraire $N(t)$ and has a special meaning: it will be used to compare the values of the other assets. The measure $P$ is called the real world measure.

\paragraph{Martingale measure:} A martingale measure for a market model is a measure $\mathbb{Q}$ on $(\Omega,\mathcal{F})$ such that the value of each of the model's assets $A_i(t)/N(t)$ relative to the numeraire is a martingale, i.e.

$$ \frac{A_i(t)}{N(t)}=\EV[\mathbb{Q}]{\frac{A_i(T)}{N(T)}|\mathcal{F}_t},$$

where $\EV[\mathbb{Q}]{}$ denotes the expectation with respect to the measure $\mathbb{Q}.$

\paragraph{Equivalent measures:} Two measures measures on $(\Omega,\mathcal{F})$ are equivalent if they agree on the possible scenarios for the assets. This means that events with probability $0$ according to one measure also have probability $0$ according to the other measure.  

\paragraph{derivative instrument:} A derivative in a market model is any financial instrument whose payoff is uniquely determined by the assets. 

\paragraph{arbitrage free:}  The model is called arbitrage free if prices of derivatives are such that there exists no self-financing strategy using the assets and numeraire yielding a certain positive profit.

\paragraph{risk neutral measure:}  A risk neutral measure with respect to a model is a martingale measure $\mathbb{Q}$ that is equivalent to the measure $P.$ 

\paragraph{complete model:} A market model is said to be complete if the payoff of any derivative can be obtained by a replicating strategy using the assets and numeraire.

\paragraph{First Fundamental Asset Pricing Theorem:} A market model is arbitrage free if and only if there exists a martingale measure equivalent to the real world measure. Such a measure is called a risk neutral measure (for the model). In such a model the value $V(t)$ of any derivative relative to the numeraire equals its expected value under the risk neutral measure:

$$ \frac{V(t)}{N(t)}=\EV[\mathbb{Q}]{\frac{V(T)}{N(T)}|\mathcal{F}_t}.$$


\paragraph{Second Fundamental Asset Pricing Theorem:} A market model is complete if and only if there exists exactly one risk neutral measure equivalent to the real world measure.

In practice the assets are often $T$-maturity riskfree zero coupon bonds and stocks with current values $P(0,T)$ and $S(0)$. The numeraire is often the risk free bank account. You then set up the model directly in the risk neutral world such that 

\begin{itemize}

\item 

It recovers the riskfree zero coupon bond prices 


$$ \EV[\mathbb{Q}]{e^{-\int_0^T r(s) ds}|\mathcal{F}_0}=P(0,T).$$

where $$\EV[\mathbb{Q}]{}$$ denotes the expectation according to the risk neutral measure, $r(s)$ denotes the short rate at time $s$, $\mathcal{F}_0$ denotes the information at the current time $0$.

\item It recovers the current stock prices 

$$ \EV[\mathbb{Q}]{S(T) e^{-\int_0^T r(s) ds}|\mathcal{F}_0}=S(0).$$

\end{itemize}

The condition on stock prices can be fullfilled by using the riskfree rate as drift for the stock price. In case you also need to model risky bonds it is necessary to model spreads as well and such that you recover the current value of the risky bonds. In case you need to model instruments in different currencies you need to recover forward FX rates and foreign riskfree zero coupon bonds as well.

Often one chooses a model from a parametrized family. Some parameters are fixed by the risk neutral conditions above. Other parameters are still free to choose. If possible, they are chosen to match prices of derivatives on the market assets. Otherwise one uses expert opinion or historical information to fix them. Once all parameters are fixed, you can use the model to price any derivative on the market assets. 

As an example consider geometric Brownian motion for a stock. This model has two parameters: the drift and volatility. The drift must be taken equal to the riskfree rate in order to have a risk neutral model. One can then use a plain vanilla call option as extra calibration instrument to fix the volatility. Usually the calibration instrument is chosen such that is as similar as possible to the instrument one finally wants to price. For instance, if one wants to price a lookback call option with a given maturity and strike, one may take the plain vanilla call option with same strike and maturity as calibration instrument.

\begin{exercise} Consider a discrete time model with two times: $0$ and $1$ year. As numeraire we take a $1$-year unit notional riskfree zero coupon bond $Z$. We assume the current value of the bond $Z(0)=1$ (i.e. the riskfree rate is $0.$) the other asset in the model is a risky asset $S(t)$ that has start value $S(0)=1$ and has two possible values at time $1$: either $S(1)=2$ or $S(1)=\frac{1}{2}$. Each of these events has probability $\frac{1}{2}$ in the real world. Show that there exists a unique equivalent martingale measure and determine the upward and downward risk neutral probabilities. Next consider a derivative instrument $V$ on $S$ such that $V(1)=1$ if $S(1)=2$ and $V(1)=0$ otherwise. Show that $V(0)=\frac{1}{3}.$ Show how to replicate the derivative by buying and/or short selling the numeraire and the risky asset at time $0$.
\end{exercise}

\begin{exercise} Consider a similar discrete time model as in the previous exercise, but assume now that $S(1)$ has three possible values $2,1,\frac{1}{2}$ each with real world probability $\frac{1}{3}$. Show that there exist several equivalent martingale measures and determine their probabilities. Consider again a derivative $V$ where $V(1)=1$ if $S(1)=2$ and $V(1)=0$ otherwise. Show that all values $V(0)$ corresponding to these equivalent martingale measures lie in the interval $]0,\frac{1}{3}[.$ Show that values in this interval cannot be arbitraged, but values outside the interval can.  
\end{exercise}

\begin{exercise}If the numeraire in a market model is a riskfree instrument, show that the expected return (according to an equivalent martingale measure) on any derivative instrument then equals the risk free rate.
\end{exercise}

\section{Partial Differential Equation for a claim}

Let $x(t)$ denote some financial tradable that follows a SDE 

$$ dx(t)=\mu(x,t)dt+\sigma(x,t)dB(t),$$

where $B(t)$ is Brownian motion with respect to a risk neutral measure $\mathbb{Q}$ (where the numeraire is the bank account process $\exp (\int_0^t r(u)du).$)

Here the drift $\mu(x,t)$ and volatility $\sigma(x,t)$ of the process are smooth deterministic functions in two variables. Let $g(x)$ denote the payoff of a claim on $x$ at a future time $T$. The value of the claim at time $t\leq T$ with $x(t)=x$ then equals 
$$ f(x,t)=\EV[\mathbb{Q}]{g(x(T))e^{-\int_t^T r(u)du} |x(t)=x}.$$

By It\^{o}'s lemma we have

\begin{eqnarray*}
 df & = & \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial x} dx + \frac{1}{2} \frac{\partial^2 f}{\partial x^2} (dx)^2 \\
 &=& \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial x} \mu(x,t) dt + \frac{\partial f}{\partial x} \sigma(x,t) dB(t) +  \frac{1}{2} \frac{\partial^2 f}{\partial x^2} \sigma^2(x,t) dt 
\end{eqnarray*}
 
 In the risk neutral world the expected return on the financial tradable and claim value must equal the risk free rate $r(t)$. Hence we get 
 
 $$ \mu(x,t)=r(t) x(t),$$
 
and we get an ordinary partial differential equation for $f$ 
 
 $$ \frac{1}{dt}\frac{1}{f}\EV[\mathbb{Q}]{df} = r(t) = \frac{1}{f}\left( \frac{\partial f}{\partial t} + \frac{\partial f}{\partial x} \mu(x,t) +  \frac{1}{2} \frac{\partial^2 f}{\partial x^2} \sigma^2(x,t)\right)$$
 
with boundary condition

$$ f(x,T)=g(x).$$ This equation is sometimes called the Feynman-Kac equation.

\section{Partial differential equation for the density}

Let $x(t)$ be as in the previous subsection. Let $\rho(x,t)$ denote the density function of $x(t)$ given $x(0)$. Let $f:\mathbb{R} \to \mathbb{R}$ be any function in one variable $x$ with continuous first and second order partial derivatives for which $f(x)$ and $\partial f / \partial x$ go to $0$ when $|x|$ goes to $\infty$. According to It\^{o}'s lemma

\begin{eqnarray*}
df(x(t))&=&\frac{\partial f}{\partial x} dx(t) + \frac{1}{2} \frac{\partial^2 f}{\partial x^2} (dx(t))^2 \\
&=&\frac{\partial f}{\partial x} (\mu(x,t) dt + \sigma(x,t) dB(t)) + \frac{1}{2} \frac{\partial^2 f}{\partial x^2} \sigma^2(x,t) dt \end{eqnarray*}

Taking expectations, we get

\begin{eqnarray*}
\EV[\mathbb{Q}]{df(x(t))} = \EV[\mathbb{Q}]{\frac{\partial f}{\partial x} \mu(x,t) dt} + \frac{1}{2} \EV[\mathbb{Q}]{\frac{\partial^2 f}{\partial x^2} \sigma^2(x,t) dt} 
\end{eqnarray*}

Hence (using integration by parts)

\begin{eqnarray*}
\int_{-\infty}^{\infty} f(x) \frac{\partial \rho}{\partial t} (x,t) dx &=& \int_{-\infty}^{\infty} \frac{\partial f}{\partial x} \mu(x,t) \rho(x,t) dx + \frac{1}{2} \int_{-\infty}^{\infty}  \frac{\partial^2 f}{\partial x^2} \sigma^2(x,t)  \rho(x,t) dx \\
&=& -\int_{-\infty}^{\infty} f(x) \frac{\partial (\mu(x,t) \rho(x,t))}{\partial x} dx + \frac{1}{2} \int_{-\infty}^\infty  f(x) \frac{\partial^2 (\sigma^2(x,t)  \rho(x,t))}{\partial x^2} dx
\end{eqnarray*}

As this equation needs to hold for any function $f(x)$ satisfying the requirements above, $\rho(x,t)$ must satisfy the following PDE

\begin{eqnarray*}
\frac{\partial \rho}{\partial t}=-\frac{\partial (\mu \rho)}{\partial x}+\frac{1}{2} \frac{\partial (\sigma^2 \rho)}{\partial x^2}
\end{eqnarray*}

This equation is called the Focker-Planck equation.

\begin{exercise} Explain why 
\[ 
\EV[\mathbb{Q}]{df(x(t))} = \int_{-\infty}^{\infty} f(x) \frac{\partial \rho}{\partial t} (x,t) dx \]
\end{exercise}

\section{Girsanov's Theorem}

\paragraph{Girsanov's theorem:} Let $B(t)$ be Brownian motion on a probability space $(\Omega,\mathcal{F},P).$ Let $X(t)$ be a stochastic process on this space adapted to the natural filtration $\mathcal{F}_t$ of $B(t).$ Define the Doleans-Dade exponential of $X(t)$ by 
$$\mathcal{E}(X)_t= e^{X(t)-\frac{1}{2}[X]_t}.$$
If $\mathcal{E}(X)_t$ is a strictly positive martingale, a probability measure $Q$ on $(\Omega,\mathcal{F})$ can be defined such that its Radon-Nykodim derivative equals
$$ \frac{dQ}{dP}|\mathcal{F}_t = \mathcal{E}(X)_t$$ and 
$Q$ is equivalent to $P$ on each $\mathcal{F}_t$. The opposite also holds. Furthermore, if $Y^P_t$ is a $P$-local martingale on $(\Omega,\mathcal{F},P,\mathcal{F}_t)$ , then 
$$ Y^Q_t=Y^P_t-[X,Y^P]_t$$ 
is a $Q$-local martingale on $(\Omega,\mathcal{F},Q,\mathcal{F}_t).$
In the special case that $Y^P_t$ is a $P$-Brownian motion and $X_t$ is continuous, $Y^Q_t$  is 
$Q$-Brownian motion.

This theorem is often applied to the case when 

$$ X_t=\int_0^t \alpha(s) dB(s).$$

Here $\alpha$ does not need to be a deterministic function of time (it can be a stochastic process as long as the integral can be defined). In the special case that $(B,Y^P_t)$ is joint $P$-Brownian motion with correlation $\rho(t)$ and $\alpha(s)$ is deterministic then we have

$$ dY^Q(t)+\alpha(t)\rho(t)dt=dY^P(t).$$

In other words, under $Q$ the process $Y^P(t)$ is Brownian motion plus drift, where the drift coefficient is given by $\alpha(t) \rho(t).$

\begin{exercise} Show that $[X]_t=\int_0^t \alpha(s)^2 ds$ when $ X_t=\int_0^t \alpha(s) dB(s)$ and show that then $\mathcal{E}(X)_t$ is indeed a martingale.
\end{exercise}

\begin{exercise} Let $B(t)$ be a Brownian motion on $(\Omega,\mathcal{F},P,\mathcal{F}_t)$. Show directly that $B(t)-\mu t$ is Brownian motion on 
$(\Omega,\mathcal{F},Q,\mathcal{F}_t)$ where $$\frac{dQ}{dP}|\mathcal{F}_t=e^{\mu B(t)- \frac{\mu^2 t}{2}}.$$ 
\end{exercise}

\begin{exercise} Let $r_d(s)$ and $r_f(s)$ denote the riskfree short rate in a domestic and foreign currency. Let $X(t)$ denote the value of one unit of the foreign currency expressed in units of the domestic currency. Let $$ B_d(t) =e^{\int_0^t r_d(s) ds} $$ and $$ B_f(t) =e^{\int_0^t r_f(s) ds} $$ denote the corresponding riskfree bank accounts. Let $P_d$ and $P_f$ denote risk neutral measures associated to these bank account numeraires. 
Assume $B_d(0)=B_f(0)=X(0)=1$ for sake of exposition. Assume 
$$ \frac{dX(t)}{X(t)} = (r_d(t)-r_f(t))dt + \sigma_X dW^{P_d}(t) $$ where $W^{P_d}(t)$ is $P_d$-Brownian motion. Let $Y(t)=X(t)B_f(t)/B_d(t).$ Let $S(t)$ denote the value of a foreign asset in units of the foreign currency. Assume that 
$$ \frac{dS(t)}{S(t)} = \mu dt + \sigma_S dB^{P_f}(t) $$ with $[B^{P_f}(t),W^{P_d}(t)]=\rho t$ where $B^{P_f}(t)$ is $P_f$-Brownian motion. 

Show that 
\begin{enumerate}
\item $$ \frac{dP_f}{dP_d}|\mathcal{F}_t = Y(t)$$
\item $$ Y(t)=\mathcal{E}\left( \sigma_X W^P(t)\right) $$
\item $$ B^{P_f}(t) = B^{P_d}(t) - \sigma_X \rho t $$ where $B^{P_d}(t)$ is $P_d$-Brownian motion. 
\item $$ \frac{dS(t)}{S(t)} = (\mu-\rho \sigma_X \sigma_S) dt + \sigma_S dB^{P_d}(t) $$ 
\end{enumerate}

\end{exercise}






A multivariate extension of Girsanov's theorem leads to the following result that can be used as a toolkit for changing numeraires in case of Brownian motions.

\paragraph{Change of numeraire toolkit:}
Let $X(t)=(X_i(t))$ be an $n$-dimensional process satisfying 
$$ \frac{dX_i(t)}{X_i(t)}=\mu_i^S(t)dt + \sigma_i(t) C_i dB^S(t), $$
where the volatility coefficients $\sigma_i(t)$ are deterministic functions of time, the $C_i$ are $1\times n$ vectors and $B^S(t)$ is $n \times 1$ standard Brownian motion under the measure associated to a numeraire $S$. Here the matrix $C$ with the $C_i$ as rows is used to induce a correlation matrix $\rho=CC'.$ 
Under another numeraire $U$ we then have (by Girsanov's theorem)
$$ \frac{dX_i(t)}{X_i(t)}=\mu_i^U(t)dt + \sigma_i(t) C_i dB^U(t)$$
for a standard Brownian motion $B^U(t)$ under the measure associated to $U$. 
Then 

$$ d\mu_i^U(t)=d\mu_i^S(t)-d\left\langle \ln X_i(t),\ln (S/U) \right\rangle.$$

Note that the drift coefficients $\mu_i^S(t)$ and $\mu_i^U(t)$ may be stochastic.
 



\chapter{Basic models}


\section{Geometric Brownian motion}

Geometric Brownian motion $S(t)$ is usually introduced via a SDE (Stochastic differential equation):

$$ d(S(t))= \mu(t) S(t) dt + \sigma(t) S(t) dB(t).$$

This actually means that $S(t)$ must solve the stochastic It\^{o} integral equation

$$ S(t)= \int_0^t \mu(s) S(s) ds + \int_0^t \sigma(s) S(s) dB(s).$$

Here $\mu(t)$ and $\sigma(t)$ are assumed to be deterministic functions of time. Defining the logarithmic return process $X(t)=\textup{ln}(\frac{S(t)}{S(0)})$ we get (using It\^{o}'s lemma)

\begin{eqnarray*}
dX(t) & = & \frac{S(0)}{S(t)} d(\frac{S(t)}{S(0)}) - \frac{1}{2} \frac{S^2(0)}{S^2(t)} (d(\frac{S(t)}{S(0)}))^2 \\
 & = & (\mu(t) -\frac{1}{2} \sigma^2(t)) dt + \sigma(t) dB(t)
\end{eqnarray*}

It follows that 

$$ X(t)=\int_0^t (\mu(s) -\frac{1}{2} \sigma^2(s)) ds + \int_0^t \sigma(s) dB(s)$$

and

$$ S(t)=S(0) \textup{exp}\left(\int_0^t(\mu(s) -\frac{1}{2} \sigma^2(s)) ds) + \int_0^t \sigma(s) dB(s)\right).$$

\begin{figure}[ht]%
\centering
%trim option's parameter order: left bottom right top
\includegraphics[trim =0mm 0mm 0mm 0mm, clip, width=\textwidth]{FIGURES/gbm_5.jpg}%
\caption{5 sample paths of geometric Brownian motion}%
\label{f:gbm_5}%
\end{figure}

\begin{figure}[ht]%
\centering
%trim option's parameter order: left bottom right top
\includegraphics[trim =0mm 0mm 0mm 0mm, clip, width=\textwidth]{FIGURES/gbm_100.jpg}%
\caption{100 sample paths of geometric Brownian motion}%
\label{f:gbm_100}%
\end{figure}

This model leads to the famous Black-Scholes formula for pricing call (or put) options (see \cite{BlackScholes}). Indeed, we can immediately assume that $B(t)$ is Brownian motion with respect to the risk neutral measure, i.e. the measure $Q$ associated to the bank account with value 
$$ C(t)=e^{\int_0^t r(s)ds},$$ where $r(s)$ denotes the risk free short rate (assumed to be deterministic). 

\begin{exercise} Show that 

$$ \int_0^t \sigma(s) dB(s) \sim N(0,\int_0^t \sigma^2(s) ds).$$ 
\end{exercise}

\begin{exercise} Show that 

$$ \EV[Q]{\textup{exp}\left(-\frac{1}{2} \int_0^t \sigma^2(s) ds + \int_0^t \sigma(s) B(s)\right)}=1.$$
Hint: show that $$E(e^X)=e^{\mu+\frac{1}{2}\sigma^2} $$ for a $N(\mu,\sigma^2)$ distributed normal random variable $X.$
\end{exercise}

\vspace{1cm}
Then we must have

$$ S(0)=\EV[Q]{S(t)/C(t)}=S(0)e^{\int_0^t (\mu(s) - r(s)) ds},$$

and hence the drift $\mu(t)=r(t)$. The payoff of a call option with strike $K$ maturing at time $T$ equals $$ \max (S(T)-K,0),$$
such that the value of the call at time $0$ equals

\begin{eqnarray*}
V(0)&=&\EV[Q]{\max (S(T)-K,0)/C(T)} \\
&=&e^{-\int_0^Tr(t)dt} \EV[Q]{\max (S(0)e^{\int_0^T (r(t)-\frac{1}{2}\sigma^2(t)) dt + \int_0^T \sigma(s) dB(s)}-K,0)} \\
&=&e^{-\int_0^Tr(t)dt}  \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty}  \max (S(0)e^{\int_0^T r(t)dt -\frac{1}{2}\Sigma^2(T)T + x\Sigma(T)\sqrt{T} }-K,0) e^{-\frac{x^2}{2}} dx \\
&=&S(0)N(d_+)-e^{-\int_0^Tr(t)dt}KN(d_-) 
\end{eqnarray*}

where $$d_\pm = \frac{ \ln (S(0)/K) + \int_0^T r(t) dt \pm T \Sigma^2(T)/2}{\Sigma(T) \sqrt{T}},$$
$$ T \Sigma^2(T)=\int_0^T \sigma^2(s) ds$$ and $N$ denotes the standard normal distribution function. Here $\Sigma(T)$ is often called the Black-Scholes volatility.

The value of $\Sigma(T)$ corresponding to the market price of a call option with strike $T$ and maturity $T$ is often called the Black-Scholes implied volatility and denoted by $\Sigma(T,K).$ 











 




 

\section{Vasicek's short rate model}

We refer to \cite{BrigoMercurio} for a detailed analysis of Vasicek's model or to the original paper of Vasicek \cite{Vasicek}.

Vasicek's model is a standard choice to model interest rates. It is expressed in terms of the short rate 

$$ r(t)=\lim_{dt \to 0} R(t,t+dt)$$

where $R(t,T)$ denotes the continuously compounded zero coupon rate for maturity $T$ at time $t.$ The model is then defined by the SDE

$$ dr(t)=\kappa(\theta - r(t))dt+\sigma dB(t)$$

where $B(t)$ denotes standard Brownian motion. 

The model has the following features
\begin{itemize}
\item Analytic formulas for bond prices and bond option prices in terms of short rate
\item Exact simulation of short rate and integrated short rate possible
\end{itemize}

\begin{figure}[ht]%
\centering
%trim option's parameter order: left bottom right top
\includegraphics[trim =0mm 0mm 0mm 0mm, clip, width=\textwidth]{FIGURES/vasicek_5.jpg}%
\caption{5 sample paths of a Vasicek process}%
\label{f:vasicek_5}%
\end{figure}

\begin{figure}[ht]%
\centering
%trim option's parameter order: left bottom right top
\includegraphics[trim =0mm 0mm 0mm 0mm, clip, width=\textwidth]{FIGURES/vasicek_100.jpg}%
\caption{100 sample paths of a Vasicek process}%
\label{f:vasicek_100}%
\end{figure}

The first feature allows us to calibrate the model parameters to bond prices and bond option prices. Due to the fact that the model has only three parameters, the calibration will be far from perfect though.

The second feature (combined with the first feature) allows us to value most interest rate derivatives by Monte Carlo simulation. The integrated short rate is needed to determine the discount factor to be applied to future cash flows, whereas the short rate itself can be used to determine the interest rate curve at any future time step. So, you can value any derivative whose payoffs at times $t$ are determined by the evolution of the interest rate curve up to time $t$. 


Taking expectations we get

$$ d\EV{r(t)|r(s)}=\kappa(\theta - \EV{r(t)|r(s)})dt,$$

and we may solve

$$ \EV{r(t)|r(s)}=\theta + (r(s)-\theta) e^{-\kappa (t-s)}.$$

This clearly shows that on average the process $r(t)$ reverts to its mean level $\theta$ when $t \to \infty.$ Hence $\theta$ is called the mean reversion level and $\kappa$ is called the mean reversion speed. We also define the halvation time $\tau$ by setting

$$  e^{-\kappa \tau} = \frac{1}{2}.$$


To solve the SDE set $x(t)=r(t)-\theta$. The SDE can then be rewritten as 

$$ dx(t)=-\kappa x(t) dt +\sigma dB(t).$$


Using It\^{o}'s lemma we get 

\begin{eqnarray*}
d(x(t) e^{\kappa t}) & =& \kappa e^{\kappa t} x(t) dt + e^{\kappa t} dx(t) \\
&=& \sigma e^{\kappa t} dB(t)
\end{eqnarray*}

So 

\begin{eqnarray*}
x(t) e^{\kappa t} & = & x(s)e^{\kappa s}+ \sigma \int_s^t e^{\kappa u} dB(u) \\
x(t) & = & x(s)e^{- \kappa (t-s)}+ \sigma \int_s^t e^{-\kappa (t-u)} dB(u) \\
r(t) & = & \theta + (r(s)-\theta) e^{- \kappa (t-s)}+ \sigma \int_s^t e^{-\kappa (t-u)} dB(u) \\
& = & r(s) e^{- \kappa (t-s)}+  \theta (1-e^{- \kappa (t-s)}) + \sigma \int_s^t e^{-\kappa (t-u)} dB(u) 
\end{eqnarray*}

From the It\^{o} isometry it now follows that 

$$ r(t)|r(s) \sim N \left( \theta + (r(s)-\theta) e^{- \kappa (t-s)}, \frac{\sigma^2}{2 \kappa} (1-e^{-2 \kappa (t-s)}) \right).$$

Integrating $r(u)$ over $[t,T]$ we get 

$$ \int_t^T r(u)du = r(t)B(t,T)+\kappa \theta \int_t^T B(t,u) du + \sigma \int_t^T B(u,T) dB(u),$$

where $$B(t,T)=\int_t^T e^{-\kappa(u-t)} du=\frac{1-e^{-\kappa (T-t)}}{\kappa}.$$
 
It follows that 

$$ \int_t^T r(u)du|r(t) \sim N \left( E(t,T), V(t,T) \right),$$

where

\begin{eqnarray*}
E(t,T) &= & r(t)B(t,T)+ \theta \left( T-t-B(t,T) \right)  \\
V(t,T) & =& \sigma^2 \int_t^T B(u,T)^2 du \\
&= & \frac{\sigma^2}{\kappa^2} \left(T-t+\frac{2}{\kappa}e^{-\kappa (T-t)}-\frac{e^{-2\kappa (T-t)}}{2\kappa} -\frac{3}{2\kappa}\right) \\
&=&\frac{\sigma^2}{\kappa^2}\left(T-t-B(t,T)-\frac{\kappa}{2}B(t,T)^2\right)
\end{eqnarray*}

It is clear also the joint distribution of $r(T)$ and $\int_t^T r(s)ds$ conditional on $r(t)$ is a normal distribution and one can obtain analytic expressions for its covariance matrix. This enables us to simulate both the short rate and its integral over any time interval exactly (without need to discretise). 

\begin{exercise} Explain why the joint distribution of $r(T)$ and $\int_0^T r(s)ds$ is a bivariate normal distribution and compute the covariance

$$ \textup{Cov}(r(T),\int_0^T r(s)ds)=\frac{\sigma^2}{\kappa^2}\left(\frac{1}{2}+\frac{1}{2}e^{-2\kappa T}-e^{-\kappa T}\right).$$
\end{exercise}
 
\paragraph{}

The zero coupon bond price at time $t$ is given by

\begin{eqnarray*}
P(t,T) &= & \EV{e^{-\int_t^T r(s) ds}|r(t)} \\
&=& e^{-E(t,T)+\frac{V(t,T)}{2}} \\
&=& A(t,T) e^{-B(t,T) r(t)}
\end{eqnarray*} 

where 

\begin{eqnarray*}
A(t,T) & = & \exp \left\{ (\theta - \frac{\sigma^2}{2 \kappa^2}) (B(t,T)-T+t) - \frac{\sigma^2}{4 \kappa} B(t,T)^2 \right\} 
\end{eqnarray*}

This means that we can easily derive the complete yield curve at any future time from the short rate at that time.


We may also obtain the zero coupon bond values more directly by using the Feynman-Kac equation:

\begin{eqnarray*}
%r(t)P(t,T)& =& \frac{\partial P}{\partial t} +\frac{\partial P}{\partial r} \kappa (\theta - r) + \frac{1}{2} \frac{\partial^2 P}{\partial r^2} (dr)^2 \\
r(t)P(t,T)&=& \frac{\partial P}{\partial t} +\frac{\partial P}{\partial r} \kappa (\theta - r) + \frac{1}{2} \frac{\partial^2 P}{\partial r^2} \sigma^2 
\end{eqnarray*}

Assuming that $P(t,T)$ can be written as 

$$ P(t,T)= A(t,T) e^{-B(t,T) r(t)} $$

for smooth functions $A(t,T)$ and $B(t,T)$, we get

$$ r(t)P(t,T)=\frac{{\partial A(t,T)}/{\partial t}}{A(t,T)}P(t,T) - \frac{\partial B(t,T)}{\partial t} r(t)P(t,T) -B(t,T)P(t,T)\kappa(\theta-r(t))
 + \frac{1}{2}  \sigma^2 B(t,T)^2 P(t,T) $$
 
Since this equation needs to hold for all $r(t)$ we get a system of ordiniray PDEs:

\begin{eqnarray*}
1&=&  - \frac{\partial B(t,T)}{\partial t} + B(t,T)\kappa \\
0&=& \frac{{\partial A(t,T)}/{\partial t}}{A(t,T)} -B(t,T)\kappa \theta + \frac{1}{2} \sigma^2 B(t,T)^2
\end{eqnarray*}

These equations can easily be solved by standard techniques and of course their solution is the same as above.

\section{Hull-White's model}

We refer to \cite{BrigoMercurio} or the original paper of Hull and White \cite{HullWhite} for more information.

In Vasicek's model the zero coupon bond prices at the current time $0$ do not need to agree with the observed zero coupon bond prices in the market. Hull-White's model is an extension of Vasicek's model where the constant $\theta$ is replaced by a deterministic function $\theta(t)$. This function is chosen such that we recover the market's zero coupon bond prices at time $0.$ The model is specified by the SDE
 
$$ dr(t)=\kappa(\theta(t) - r(t))dt+\sigma dB(t)$$

where $$
\theta(t)=f^M(0,t) + \frac{1}{\kappa} \frac{\partial f^M(0,t)}{\partial t}+\frac{\sigma^2}{2\kappa^2} \left(1- e^{-2\kappa t} \right) $$

Here 

$$ f^M(0,t)=-\frac{\partial \ln P^M(0,t)}{\partial t}$$

denotes the market's current instantaneous forward rates and

$$ P^M(0,t)=e^{-\int_0^t f^M(0,u) du}$$

denote the market's zero coupon bond prices. It is assumed that the full curve is known. Usually this curve is obtained by interpolating observed zero coupon rates. The interpolated zero-coupon rate curve must be smooth in order to guarantee the existence of the first derivatives.

The Hull-White model has the same attractive features as Vasicek's model plus market consistency with bond prices:

The model has the following features
\begin{itemize}
\item Analytic formulas for bond prices and bond option prices in terms of short rate
\item Exact simulation of short rate and integrated short rate possible
\item Exact valuation of the market's bond prices
\end{itemize}


We now derive the formulae. We start by following the same lines as for Vasicek's model but assuming $\theta(t)$ is any smooth deterministic function. Later we will fix $\theta(t)$ to match the market's zero coupon bond prices.

Setting $x(t)=r(t)-\theta(t)$ we can use It\^{o}'s lemma to obtain

$$ x(t)  =  x(s)e^{- \kappa (t-s)} -\int_s^t e^{-\kappa (t-u)} d\theta(u) + \sigma \int_s^t e^{-\kappa (t-u)} dB(u) $$

By substituting $r(t)-\theta(t)$ for $x(t)$ and using partial integration we get 

$$ r(t)=r(s)e^{-\kappa (t-s)} + \kappa \int_s^t \theta(u) e^{-\kappa(t-u)} du + \sigma \int_s^t e^{-\kappa (t-u)} dB(u) $$

Integrating the short rate over $[t,T]$ we get

$$ \int_t^T r(u) du = r(t)B(t,T)+\kappa \int_t^T \theta(u) B(u,T) du + \sigma \int_t^T B(u,T) dB(u) $$

where $$B(t,T)=\int_t^T e^{-\kappa(T-u)} du=\frac{1-e^{-\kappa (T-t)}}{\kappa}.$$
 
It follows that 

$$ \int_t^T r(u)du|r(t) \sim N \left( E(t,T), V(t,T) \right),$$

where

\begin{eqnarray*}
E(t,T) &= & r(t)B(t,T)+\kappa \int_t^T \theta(u) B(u,T) du \\
V(t,T) & =& \sigma^2 \int_t^T B(u,T)^2 du \\
&= & \frac{\sigma^2}{\kappa^2} \left(T-t+\frac{2}{\kappa}e^{-\kappa (T-t)}-\frac{e^{-2\kappa (T-t)}}{2\kappa} -\frac{3}{2\kappa}\right) \\
&=&\frac{\sigma^2}{\kappa^2}\left(T-t-B(t,T)-\frac{\kappa}{2}B(t,T)^2\right)
\end{eqnarray*}

The zero coupon bond prices at time $t$ are given by

\begin{eqnarray*}
P(t,T) &= & \EV{e^{-\int_t^T r(s) ds}|r(t)} \\
&=& e^{-E(t,T)+\frac{V(t,T)}{2}} 
\end{eqnarray*} 

We can now use this formula to determine $\theta(t)$ such that the model reproduces the market's zero coupon bond prices at time $0.$ Let

$$ f^M(0,t)=-\frac{\partial \ln P^M(0,t)}{\partial t}$$

denote the market's current instantaneous forward rates. Then

\begin{eqnarray}
f^M(0,t) &= &-\frac{\partial \ln P^M(0,t)}{\partial t} \\
&=&\frac{\partial E(0,t)}{\partial t}-\frac{1}{2} \frac{\partial V(0,t)}{\partial t} \\
&=& r(0)e^{-\kappa t} + \kappa e^{-\kappa t} \int_0^t \theta(u) e^{\kappa u} du  -\frac{1}{2} \frac{\partial V(0,t)}{\partial t} \label{eq:HullWhite}
\end{eqnarray}

Taking the partial derivative with respect to $t$ we get
\begin{eqnarray*}
	\frac{\partial f^M(0,t)}{\partial t} & =&-\kappa \left( f^M(0,t)+\frac{1}{2} \frac{\partial V(0,t)}{\partial t} \right) 
	+ \theta(t) \kappa - \frac{1}{2} \frac{\partial^2 V(0,t)}{\partial t^2} 
\end{eqnarray*}

Using

\begin{eqnarray*}
	\frac{\partial V(0,t)}{\partial t} &= &\frac{\sigma^2}{\kappa^2} \left( 1-2e^{-\kappa t}+e^{-2\kappa t} \right) \\
	\frac{\partial^2 V(0,t)}{\partial t^2} &=&\frac{\sigma^2}{\kappa^2} \left( 2\kappa e^{-\kappa t} -2\kappa e^{-2\kappa t} \right)
\end{eqnarray*}

we get 

\begin{eqnarray*}
	\theta(t)=f^M(0,t) + \frac{1}{\kappa} \frac{\partial f^M(0,t)}{\partial t}+\frac{\sigma^2}{2\kappa^2} \left(1- e^{-2\kappa t} \right)
\end{eqnarray*}

Setting $$\alpha(t)=f^M(0,t)+\frac{1}{2} \frac{\partial V(0,t)}{\partial t}$$

from \ref{eq:HullWhite} we get

\begin{eqnarray*}
\kappa \int_t^T \theta(u) e^{-\kappa (T-u)} du &=& \kappa \int_0^T \theta(u) e^{-\kappa (T-u)} du - \kappa \int_0^t \theta(u) e^{-\kappa (T-u)} du \\
&=&\alpha(T)+r(0)e^{-\kappa T} -e^{-\kappa (T-t)} (\alpha(t)+r(0)e^{-\kappa t}) \\
&=&\alpha(T)-e^{-\kappa (T-t)} \alpha(t)
\end{eqnarray*}

On the other hand

\begin{eqnarray*}
\int_t^T \theta(u) du &=& \int_t^T \left( f^M(0,u) + \frac{1}{\kappa} \frac{\partial f^M(0,u)}{\partial t}+\frac{\sigma^2}{2\kappa^2} (1- e^{-2\kappa u}) \right) du\\
&=& \ln \frac{P^M(0,t)}{P^M(0,T)} + \frac{1}{\kappa} \left( f^M(0,T)-f^M(0,t) \right) + \frac{\sigma^2}{2\kappa^2} \int_t^T (1- e^{-2\kappa u}) du 
\end{eqnarray*}

Hence 
\begin{eqnarray*}
E(t,T)&=&B(t,T)r(t)+ \int_t^T \theta(u) du - \int_t^T \theta(u) e^{-\kappa(T-u)} du \\
&=& B(t,T)(r(t)-\alpha(t))+\frac{\alpha(t)-\alpha(T)}{\kappa}+\int_t^T \theta(u) du \\
&=& B(t,T)(r(t)-\alpha(t))+\frac{f^M(0,t)-f^M(0,T)}{\kappa}+\frac{1}{2\kappa}\left(\frac{\partial V(0,t)}{\partial t}-\frac{\partial V(0,T)}{\partial T}\right)+\int_t^T \theta(u) du \\
&=& B(t,T)(r(t)-\alpha(t))+\frac{1}{2\kappa}\left(\frac{\partial V(0,t)}{\partial t}-\frac{\partial V(0,T)}{\partial T}\right)+\ln \frac{P^M(0,t)}{P^M(0,T)} + \frac{\sigma^2}{2\kappa^2} \int_t^T (1- e^{-2\kappa u}) du \\
&=& B(t,T)(r(t)-\alpha(t))+\ln \frac{P^M(0,t)}{P^M(0,T)} + \frac{1}{2}\left(V(0,T)-V(0,t)\right) \\
\end{eqnarray*}

We can check that 

$$ V(0,T)-V(0,t)-V(t,T)-B(t,T)\frac{\partial V(0,t)}{\partial t} = \frac{\sigma^2}{2\kappa}B(t,T)^2 (1-e^{-2\kappa t}).$$

Using this formula it follows that 

$$ P(t,T)=A(t,T)e^{-B(t,T)r(t)}$$

where

$$ A(t,T)=\frac{P^M(0,T)}{P^M(0,t)} \exp \left( B(t,T)f^M(0,t) -\frac{\sigma^2}{4\kappa}B(t,T)^2 (1-e^{-2\kappa t}) \right).$$

 

\section{CIR model}

We refer to \cite{BrigoMercurio} or the original paper \cite{CIR} for more details.

The Cox-Ingersol-Ross (CIR) model is a short rate model similar to Vasicek's model. However the volatity is now proportional to the square root of the rate. It follows that under suitable conditions for the parameters the short rates remain positive. It is described by the SDE

$$ dr(t)=\kappa(\theta-r(t))dt+\sigma \sqrt{r(t)} dB(t).$$

The mean reversion part is the same as in Vasicek's model. It can be shown that short rates remain positive under the condition that $2\kappa \theta > \sigma^2.$

The CIR model combines the following features
\begin{itemize}
\item Analytic formulas for bond prices and bond option prices in terms of short rate
\item Exact simulation of the short rate is possible
\item Positive short rates with positively skewed distribution
\end{itemize}

\begin{figure}[ht]%
\centering
%trim option's parameter order: left bottom right top
\includegraphics[trim =0mm 0mm 0mm 0mm, clip, width=\textwidth]{FIGURES/cir_5.jpg}%
\caption{5 sample paths of a CIR process}%
\label{f:cir_5}%
\end{figure}

\begin{figure}[ht]%
\centering
%trim option's parameter order: left bottom right top
\includegraphics[trim =0mm 0mm 0mm 0mm, clip, width=\textwidth]{FIGURES/cir_100.jpg}%
\caption{100 sample paths of a CIR process}%
\label{f:cir_100}%
\end{figure}

One can slightly extend the model allowing for negative but floored short rates by applying a constant shift. Such model is specified by 

$$ dr(t)=\kappa(\theta-r(t))dt+\sigma \sqrt{r(t)-f} dB(t),$$

where $f$ denotes the floor. Short rates are then larger than the floor $f$ that can be chosen negative.

\begin{exercise} Consider the SDE

$$ dr(t)=\kappa(\theta-r(t))dt+\sigma r(t) dB(t).$$

Show that $$ E(e^{\int_0^t r(s) ds})=\infty.$$ In other words the bank account is expected to explode: for this reason the model is not realistic.
\end{exercise}

\paragraph{}

The probability density function of the short rate $r(T)$ conditional on its value $r(t)$ at time $t$ is given by 

$$  \rho(r(T)|r(t)) = d e^{-u-v} \left(\frac{v}{u}\right)^{\frac{q}{2}} I_q(2\sqrt{uv}) $$

where 

\begin{eqnarray*}
d&=&\frac{2\kappa}{\sigma^2(1-e^{-\kappa(T-t)})} \\
u&=& dr(t)e^{-\kappa(T-t)} \\
v&=&dr(T) \\
q&=&\frac{2\kappa \theta}{\sigma^2}-1
\end{eqnarray*}

and $I_q(.)$ is the modified Bessel function of the first kind of order $q$. 

The distribution function is the noncentral chi-square, $\chi^2(2dr(T);2q+2,2u)$ with $2q+2$ degrees of freedom and parameter of noncentrality $2u$ proportional to the spot rate. We refer to the appendices for a derivation of the formula. 

The prices of zero coupon bonds $P(t,T)$ can be obtained by solving the Feynman-Kac equation for the bond price.  As in Vasicek's model we assume an affine bond price structure , i.e. 

$$ P(t,T) = A(t,T) e^{-B(t,T) r(t)} $$

for smooth functions $A(t,T)$ and $B(t,T)$.

See appendix \ref{appendix:FeynmanKac} for a derivation of the solution

\begin{eqnarray*}
B(t,T)&=& \frac{2(e^{\psi (T-t)}-1)}{(\psi+\kappa)e^{\psi (T-t)}+\psi - \kappa} \\
A(t,T)&=& \left( \frac{2\psi e^{\frac{(\psi+\kappa)(T-t)}{2}}}{(\psi+\kappa)e^{\psi(T-t)}+\psi-\kappa} \right)^\frac{2\kappa \theta}{\sigma^2},
\end{eqnarray*}
where
$$ \psi=\sqrt{\kappa^2+2\sigma^2}.$$


\section{CIR++ model}

We refer to \cite{BrigoMercurio} for more details.

The CIR++ model plays the same role for the CIR model as Hull-White's model does for Vasicek's model. It extends the model such that it recovers the market's bond prices. To ensure analytical bond prices it may be specified as follows

\begin{eqnarray*}
dx(t)&=&\kappa(\theta -x(t))dt +\sigma \sqrt{x(t)} dB(t) \\
r(t)&=&x(t)+\phi(t)
\end{eqnarray*}

with $2\kappa \theta > \sigma^2,$
where 

\begin{eqnarray*}
\phi(t)&=&f^M(0,t)-f^{CIR}(0,t) \\
f^{CIR}(0,t)&=& \frac{2\kappa \theta (e^{\psi t}-1)}{2\psi+(\psi+\kappa)(e^{\psi t}-1)}+x(0)\frac{4\psi^2e^{\psi t}}{(2\psi+(\psi+\kappa)(e^{\psi t}-1))^2}
\end{eqnarray*}

Positivity of interest rates can only be guaranteed when $\phi(t)>0$. This condition is however rather restrictive and in case of negative market forward rates it cannot be fullfilled. In calibrating the model one may choose $x(0)$ e.g. to stay as close as possible to positive rates. However, this may have a negative impact on fitting to caps and floors.  

\begin{exercise}  Show that $f^{CIR}(0,t)$ equals the current instantaneous forward rate for time $t$ in the CIR model.
\end{exercise}

\begin{exercise}  Show that we can set up the Hull-White model in a similar way where

$$ \phi(t)=f^M(0,t)-f^{VAS}(0,t) $$ and $f^{VAS}(0,t)$ denotes the current instantaneous forward rate for time $t$ in the Vasicek model.
\end{exercise}

\chapter{Interest rate market models}

We refer to \cite{BrigoMercurio} for an in-depth discussion on the interest rate market models.

\section{Caplets}

Consider two future times $0<T_1<T_2.$ Let $L(T_1,T_2)$ denote the simply compounded risk free rate fixed at time $T_1$ over the time interval $[T_1,T_2].$ This is often called the LIBOR (London InterBank Offered Rate)rate and represents the average offered interest rate at which banks are willing to trade over the time interval. It means that 

$$ \frac{1}{P(T_1,T_2)}=1+L(T_1,T_2)(T_2-T_1).$$

This rate is unknown at time $t<T_1.$ A caplet is an option on this forward rate paying off 

$$ (T_2-T_1) \max (0,L(T_1,T_2)-K)$$ at time $T_2$. 

It can be used to cap the LIBOR rate that has to be paid on a given contract to the strike rate $K$. 

At time $t<T_1$ we may consider the forward rate

$$ F(t,T_1,T_2)=\frac{1}{T_2-T_1} \left( \frac{P(t,T_1)}{P(t,T_2)}-1 \right).$$

This rate converges to the LIBOR rate at time $T_1$ and hence we can consider the caplet as an option on this forward rate.

Note that the caplet's payoff at time $T_2$ is equivalent to a payoff at time $T_1$ of

$$ P(T_1,T_2)(T_2-T_1) \max (0,L(T_1,T_2)-K).$$

This expression is suitable for pricing in the risk neutral world attached to the $T_2$-maturity zero coupon bond numeraire. We call the associated measure $\mathbb{Q}(T_2)$ the $T_2$-forward measure. The caplet value at time $t$ then equals
$$ V(t)=P(t,T_2)\EV[\mathbb{Q}(T_2)]{(T_2-T_1) \max (0,L(T_1,T_2)-K)}.$$

Now $$ P(t,T_2)F(t,T_1,T_2)=\frac{P(t,T_1)-P(t,T_2)}{T_2-T_1}$$ is the price of a tradable asset. So, if we want a market model for the yield curve its price relative to the numeraire must be a martingale. Hence $F(t,T_1,T_2)$ itself must be a martingale. 

The LIBOR forward model is then specified by setting

$$ dF(t,T_1,T_2)=\sigma(t)F(t,T_1,T_2)dW^{\mathbb{Q}(T_2)},$$

where $W^{\mathbb{Q}(T_2)}$ is Brownian motion with respect to $\mathbb{Q}(T_2).$ 

It follows that the price of the caplet may be computed analytically. This is the justification behind the so-called Black formula (analogue of the Black-Scholes formula for equity calls).

\section{LIBOR market model}

The above model specifies the dynamics of one forward rate relative to its forward measure. This is sufficient to price a caplet, or a series of caplets (cap). However, in case we want to specify a model for the complete yield curve we need to consider the dynamics of all forward rates relative to one measure. 
In the LIBOR market model the yield curve is discretised according to the maturity of the forward rates underlying the caplets, e.g. each 6 months: $0=T_0<T_1<\hdots <T_N.$ 

Denote the forward rates by $$F_k(t):=F(t,T_{k-1},T_k)=(P(t,T_{k-1})/P(t,T_k)-1)/\tau_k,$$  where $\tau_k=T_k-T_{k-1}.$ Then the complete model is specified by the set of SDEs

$$\frac{dF_k(t)}{F_k(t)}=\sigma_k(t)dB^k(t),$$

where each $B^k(t)$ is Brownian motion with respect to the $T_k$-forward measure and $[B_k,B_l]_t=\rho_{k,l} dt.$  

By the change of numeraire technique we can also express all forward rates with respect to one $T_i$-forward measure, yielding

\[ \frac{dF_k(t)}{F_k(t)}=\sigma_k(t)\sum_{j=i+1}^k \frac{\rho_{k,j} \sigma_j(t) \tau_j F_j(t)}{1+\tau_j F_j(t)} dt + \sigma_k(t) dB^k(t) \textup{ for $i<k$} \]
\[ \frac{dF_k(t)}{F_k(t)}=-\sigma_k(t)\sum_{j=k+1}^i \frac{\rho_{k,j} \sigma_j(t) \tau_j F_j(t)}{1+\tau_j F_j(t)} dt + \sigma_k(t) dB^k(t) \textup{ for $i>k$} \]

Note that the correlated Brownian motions for the forward rates can be constructed from a linear combination of just a few independent Brownian motions, called the factors. The initial yield curve is automatically reproduced in the sense that the forward rates according to the grid are respected.

The model can then be used to price any interest rate derivative with payoff times amongst the $T_k$ via Monte Carlo simulation. We need to choose the $T_i$-numeraire such that $T_i$ is larger than or equal to the final payoff of the derivative. Indeed, the value at time $t$ of a derivative with only payoff $V(T)$ at time $T<T_i$ equals

$$ V(t)=P(t,T_i)\EV[\mathbb{Q}(T_i)]{\frac{V(T)}{P(T,T_i)}}.$$

Note that due to the drift corrections it is necessary to discretise time in order to simulate the SDEs.

\section{Swap Market Model}

Consider a swap for which the floating leg pays $L(T_{j-1},T_j) \tau_j$ at times $T_j$ for $j=\alpha+1, ... \beta,$ where $L(T_{j-1},T_j)$ is the LIBOR rate fixed at $T_{j-1}.$ The fixed leg pays $K\tau_j$ at times $T_j$ where $K$ denotes the swap rate. 

At any time $t<T_\alpha$, the value of the fixed leg equals

\[ \sum_{j=\alpha+1}^\beta P(t,T_j) \tau_j K, \]

We may replicate the floating leg payoffs as follows. At time $t$ we buy a $T_\alpha$-maturity zero coupon bond and sell a $T_\beta$-maturity zero coupon bond (both with unit face value). At time $T_\alpha$ we invest the unit face value of the $T_\alpha$- bond in $T_{\alpha+1}$-zero coupon bonds. This yields the LIBOR rate over the period $\tau_{\alpha+1}$ at time $T_{\alpha+1}$ (i.e. the floating leg payoff) plus one unit of currency that we reinvest in a $T_{\alpha+2}$-zero coupon bond. We can repeat this investment strategy. At the last period we buy a unit face value $T_{\beta}$-zero coupon bond that cancels our initial short position in this bond. This replicating strategy shows that the value of the floating leg equals

$$ P(t,T_\alpha)-P(t,T_\beta).$$

The (forward) swap rate at time $t$ is the fixed rate that produces equal values for both legs:

\[ S_{\alpha,\beta}(t)=\frac{P(t,T_\alpha)-P(t,T_\beta)}{  \sum_{j=\alpha+1}^\beta P(t,T_j) \tau_j}.\]

Now consider a payer swaption, i.e. the right to enter the swap (as fixed leg payer) at $T_\alpha$ at swap rate $K$ (without up front payment). You will only exercise this right when the swap rate $S_{\alpha,\beta}(T_\alpha)\geq K.$ The value of the swaption then equals the difference in the two fixed legs (at par versus at strike $K$) i.e. 

\[ \sum_{j=\alpha+1}^{\beta} P(T_\alpha,T_j) \tau_j (S_{\alpha,\beta}(T_\alpha)-K).\]

Hence we can say that the swaption payoff at time $T_\alpha$ equals

\[ \sum_{j=\alpha+1}^{\beta} P(T_\alpha,T_j) \tau_j \max(0,S_{\alpha,\beta}(T_\alpha)-K).\]
  
It follows that \[C^{\alpha,\beta}(t)=\sum_{j=\alpha+1}^{\beta} P(t,T_j) \tau_j \]

is a suitable numeraire for pricing the swaption. Note that being a simple portfolio of zero coupon bonds it is a tradable asset. We call this numeraire the swap numeraire and denote its associated measure by $Q^{\alpha,\beta}.$

Since by definition the swap rate equals the value of a tradable asset discounted at the swap numeraire, it must be a martingale with respect to its associated risk neutral measure $Q^{\alpha,\beta}.$

The swap market model is then specified by setting

$$ dS_{\alpha,\beta}(t)=\sigma^{\alpha,\beta} S_{\alpha,\beta}(t) dB^{\alpha,\beta}(t),$$

where $B^{\alpha,\beta}(t)$ denotes Brownian motion under $Q^{\alpha,\beta}.$

The value of the swaption at time $t$ then equals

$$ V(t)=C^{\alpha,\beta}(t)\EV[Q^{\alpha,\beta}]{\max (0,S_{\alpha,\beta}(T_\alpha)-K)}.$$

It is clear that due to the model choice a Black-like formula exists for its price. 


\chapter{Heston's Model}

We refer to the original paper of Heston \cite{Heston} for more details.

\section{Volatility Smile}

Black-Scholes' Geometric Brownian Motion assumes a deterministic volatility function $\sigma(t)$. This volatility function can be calibrated to observed market prices of call options with a given strike at different maturities.
However, according to the model two call options with the same maturity $T$ but different strikes should have the same Black-Scholes volatility

$$ \frac{1}{\sqrt{T}}\sqrt{\int_0^T \sigma^2(t) dt}.$$

However, in practice it turns out that the implied volatility $\Sigma(T,K)$ also depends on the strike $K$. Plotting $\Sigma(T,K)$ in terms of the strike $K$ typically produces a smile-like graph (typical for currency options). Depending on the asset the graph may also look like a skew (typical for equity options).

Note that the volatility does no depend on whether the option is a call or put option. This is due to the put-call parity. 

The fact that volatility increases for lower strikes can be understood as follows. Suppose first that the implied BS volatiliy were a flat function of the strike. This means that the lognormal distribution with volatility $\sigma$ for the asset return conincides with the market's view. However, since the implied volatility is higher for lower strikes, it means that the market assigns relatively more value to the low strike put (compared to the flat volatility). In other words it believes that the (risk neutral) probability of reaching the strike must be higher, i.e. that the left tail of the risk neutral distribution must be fatter.

That is one of the main reasons for looking at models producing fatter tails. One of such models is Heston's model. It achieves this by assuming that the volatility itself is stochastic, a fact that has been observed in historical volatility as well as in BS implied market volatility.

\section{Heston's Model}

Heston's Model is specified by combining geometric Brownian motion for the asset price with a CIR process for the variance process:

\begin{eqnarray*}
\frac{dS(t)}{S(t)} &= & \mu(t) dt + \sigma(t) dB^S(t) \\
d\sigma^2(t) & =& \kappa (\theta - \sigma^2(t)) dt + \xi \sigma(t) dB^\sigma(t)
\end{eqnarray*}
where
\[ dB^S(t)dB^\sigma(t)=\rho dt.\]

Note that the volatility $\sigma^2(0)$ at time $0$ needs to be specified. It must be considered as an extra parameter to calibrate. The mean reverting stochastic instantaneous variance $\sigma^2(t)$ induces a volatility smile on the BS implied volatility curve in terms of the strike (when $\rho=0$). In equity markets one often sets $\rho<0$ in order to arrive at a volatility skew.

The Heston model has the following interesting features:

\begin{itemize}
\item it has volatility clustering: the variances of logarithmic returns of the asset price over consecutive time intervals are positively correlated
\item it explains the volatility smile/skew
\item it is relatively easy to simulate (by discretisation)
\item it has quasi-analytic formulae for plain vanilla call and put options
\end{itemize}

For a risk neutral Heston model $\mu(t)$ must be the riskfree rate. We are then left with 5 parameters to calibrate: $\kappa$, $\theta$, $\xi$, $\rho$ and the non-observable $\sigma(0).$ 
It is quite challenging to calibrate the model to plain vanilla call (or put) options as you need to minimize differences between modeled and observed prices in a five dimensional space. One typically minimises the sum of squares of these differences. Optimisation algorithms usually only find a local minimun of this sum function. So it may be necessary to combine them with a grid search. Moreover there may be many solutions yielding approximately the same sums with quite different parameters. In particular, the calibration may become unstable for small changes in the input call prices. Another approach first calibrates a subset of parameters before calibrating the other parameters. You may for instance use variance swaps to calibrate $\kappa$, $\theta$ and $\sigma(0)$. The call prices are then used to further calibrate $\rho$ and $\xi$. This approach may yield more stable parameters. In general it is known that the Heston model can fit the volatility smile for longer maturity options quite well. Short maturity options are harder to calibrate, since the model does not alow for jumps.  

\begin{exercise}  Show that 

$$ E\left( \frac{1}{T}  \int_0^T \sigma^2(t) dt \right) = \theta + (\sigma^2(0)-\theta) \frac{1-e^{-\kappa T}}{\kappa T}.$$
\end{exercise}  

\begin{figure}[ht]%
\centering
%trim option's parameter order: left bottom right top
\includegraphics[trim =0mm 0mm 0mm 0mm, clip, width=\textwidth]{FIGURES/heston_5.jpg}%
\caption{5 sample paths of a Heston process}%
\label{f:heston_5}%
\end{figure}

\begin{figure}[ht]%
\centering
%trim option's parameter order: left bottom right top
\includegraphics[trim =0mm 0mm 0mm 0mm, clip, width=\textwidth]{FIGURES/heston_100.jpg}%
\caption{100 sample paths of a Heston process}%
\label{f:heston_100}%
\end{figure}

\chapter{L\'{e}vy Processes}

A nice introduction to L\'{e}vy processes can be found in \cite{Schoutens}. More extensive excellent references are \cite{Sato} and \cite{ContTankov}.

\section{Poisson Process}

A Poisson process $N_{\lambda}(t)$ on a probability space $(\Omega,\mathcal{F},P)$ is a cadlag stochastic process such that its increments over disjoint time intervals are independent and follow a Poisson distribution with parameter $\lambda t$ where $t$ denotes the length of the time interval. Hence $E(N_{\lambda}(t))=\lambda t$ and we call $\lambda$ the expectation rate.

\begin{figure}[ht]%
\centering
%trim option's parameter order: left bottom right top
\includegraphics[trim =0mm 0mm 0mm 0mm, clip, width=\textwidth]{FIGURES/poisson_5.jpg}%
\caption{5 sample paths of a Poisson process}%
\label{f:poisson_5}%
\end{figure}

\begin{figure}[ht]%
\centering
%trim option's parameter order: left bottom right top
\includegraphics[trim =0mm 0mm 0mm 0mm, clip, width=\textwidth]{FIGURES/poisson_100.jpg}%
\caption{100 sample paths of a Poisson process}%
\label{f:poisson_100}%
\end{figure}

Hence 

\[ P(N_{\lambda}(t)=n)=\frac{(\lambda t)^n}{n!}e^{-\lambda t} \]

Note that the characteristic function $\Phi_{N_{\lambda}(1)}(\theta)$ of $N_{\lambda}(1)$ equals

\[  \Phi_{N_{\lambda}(1)}(\theta):=E(e^{i\theta N_{\lambda}(1)})=e^{\lambda(e^{i\theta}-1)}.\]

and moreover

\[  \Phi_{N_{\lambda}(t)}(\theta)=(\Phi_{N_{\lambda}(1)}(\theta))^t.\]

\begin{exercise}  Let $\tau$ denote the first jump time of the Poisson process $N_\lambda(t).$ Show that $\tau \sim \textup{Exp}(\lambda).$
\end{exercise}

\begin{exercise}  Let $\tau_n, n=1,2,...$ be a sequence of i.i.d $\textup{Exp}(\lambda)$ distributed random variables. Define a process $N_\lambda(t)$ by setting 

\[ N_\lambda(t)=N \Leftrightarrow \sum_{n=1}^{N} \tau_n \leq t \textup{ and } \sum_{n=1}^{N+1} \tau_n > t.\]

Show that $N_\lambda(t)$ is a Poisson process with expectation rate $\lambda$. Hint: prove by induction on $N$ that for all $N\geq 0$ \[ P(\sum_{n=1}^{N+1} \tau_n >t) = e^{-\lambda t} \left( \sum_{n=0}^{N} \frac{(\lambda t)^n}{n!} \right).\]
\end{exercise}


\section{Compound Poisson process}

A compound Poisson process $X(t)$ on a probability space $(\Omega,\mathcal{F},P)$ is like a Poisson process with jumps of different sizes according to a jump size measure $J.$
Thus

\[ X(t)=\sum_{n=1}^{N_{\lambda}(t)} J_n,\]

where $N_{\lambda}(t)$ is a Poisson process with parameter $\lambda$ and the $J_n$ are independent copies of a random variable with $P(J_n\in A)=J(A).$ Here $J$ is a jump measure and must satisfy $J(\{0\})=0.$

\begin{figure}[ht]%
\centering
%trim option's parameter order: left bottom right top
\includegraphics[trim =0mm 0mm 0mm 0mm, clip, width=\textwidth]{FIGURES/compound_poisson_5.jpg}%
\caption{5 sample paths of a compound Poisson process}%
\label{f:compound_poisson_5}%
\end{figure}

Again this process has independent increments over disjoint time intervals and the distribution of increments over time intervals of the same length is identical. 

Denote the characteristic function of $J$ by $\Phi_J(\theta).$ The characteristic function of $X(1)$ may then be computed as follows

\begin{eqnarray*}
\Phi_{X(1)}(\theta) & =& E(e^{i\theta X(1)}) \\
&=& \sum_{n=1}^\infty P(N(1)=n) \Phi_{\sum_{i=1}^n J_i}(\theta) \\
&=& \sum_{n=1}^\infty P(N(1)=n) (\Phi_J(\theta))^n \\
&=& \sum_{n=1}^\infty \frac{(\lambda \Phi_J(\theta))^n }{n!} e^{-\lambda} \\
&=& e^{\lambda (\Phi_J(\theta)-1)} \\
&=& \Phi_{N_{\lambda}(1)}(-i \ln \Phi_J(\theta) )
\end{eqnarray*}

Let $A$ be a measurable subset of $\mathbb{R}\backslash \{0\},$ and let $N_A(t)$ denote the process counting the number of jumps of $X(t)$ of size in $A.$ Then, a similar computation, yields

\begin{eqnarray*}
\Phi_{N_A(1)}(\theta)& =& E(e^{i\theta N_A(1)}) \\
&=& \sum_{n=1}^\infty P(N_\lambda(1)=n) E(e^{i \theta \sum_{j=1}^n 1_{\{J_j \in A\}}}|N_\lambda(1)=n) \\
&=& \sum_{n=1}^\infty P(N_\lambda(1)=n) (1-J(A)+J(A)e^{i\theta })^n \\
&=& e^{\lambda J(A)(e^{i\theta}-1)}
\end{eqnarray*}

Thus $N_A(t)$ is a Poisson process with expectation $\lambda J(A)t.$

\begin{exercise}  Show that a compound Poisson process reduces to a Poisson process if and only if the jump measure is the Dirac measure at $1$, i.e. $J(\{1\})=1.$
\end{exercise}

\section{L\'{e}vy  Process}

We have seen that both Brownian motion with drift and compound Poisson processes are processes with independent and identically distributed increments over disjoint time intervals of the same length. This is essentially the definition of a L\'{e}vy  process and L\'{e}vy showed that each such process is essentially the sum of Brownian motion with drift and a (slight generalisation) of a compound Poisson process. 

\paragraph{L\'{e}vy process:} A cadlag stochastic process $X(t)$ on a probability space  $(\Omega,\mathcal{F},P)$ is a L\'{e}vy process if 
\begin{itemize}
\item $X(0)=0$ almost surely
\item it has independent increments over disjoint time intervals
\item it has identical increments over time intervals of the same length
\item it is continuous in probability: for all $\epsilon>0$ and $t\geq 0$ 
\[ \lim_{dt\to 0} P(|X(t+dt)-X(t)|>\epsilon)=0.\]
\end{itemize}

\paragraph{L\'{e}vy-Khintchine representation:} The characteristic function $\Phi_{X(1)}(\theta)$ of a L\'{e}vy  process $X(t)$ is given by 

\[ \exp \left( ai \theta -\frac{1}{2}\sigma^2 \theta^2 + \int_{\mathbb{R}\backslash \{0\}} (e^{i\theta x} -1 -i\theta x 1_{\{|x|<1\}}(x)) \Pi(dx) \right) ,\]

where $a,\sigma \in \mathbb{R}$ and $\Pi$ is a measure on $(\mathbb{R},\mathcal{B}(\mathbb{R}))$ satisfying

\[ \int_{\mathbb{R}\backslash \{0\}} \min (1,x^2) \Pi(dx) < \infty .\]

The characteristic function is completely determined by the so-called L\'{e}vy  triplet $(a,\sigma,\Pi).$ 

Note the indicator function $1_{\{|x|<1\}}(x).$ It allows the density of the jump measure to increase to $\infty$ around $x=0.$ One could as well use the indicator function $1_{|x|<b}(x)$ for any $b>0$. This only changes the value of $a.$ Note that the integrand is of order $x^2$ around $x=0.$ Away from $0$ the integrand is bounded. Hence, by the extra condition, the integral exists. 

In case of a compound Poisson process with frequency parameter $\lambda$ and jump size measure $J$, we can define $\Pi=\lambda J.$ We then get:

\begin{eqnarray*}
\ln \Phi_{X(1)}(\theta)&=& \lambda (\Phi_J(\theta)-1) \\
&=& \int_{\mathbb{R}\backslash \{0\}} (e^{i\theta x}-1) \lambda J(dx) \\
& = &  i \theta \int_{\mathbb{R}\backslash \{0\}} x 1_{\{|x|<1\}}(x) \Pi(dx) \\
& & +\int_{\mathbb{R}\backslash \{0\}} (e^{i\theta x} -1 -i\theta x 1_{\{|x|<1\}}(x)) \Pi(dx) \\
&=& i\theta a + \int_{\mathbb{R}\backslash \{0\}} (e^{i\theta x} -1 -i\theta x 1_{\{|x|<1\}}(x)) \Pi(dx) 
\end{eqnarray*}

where

\[ a= \int_{\mathbb{R}\backslash \{0\}} x 1_{\{|x|<1\}}(x) \lambda J(dx).\]

The L\'{e}vy  triplet thus equals $(a,0,\lambda J).$

It is clear from the representation theorem that each L\'{e}vy  process can be decomposed into drift plus Brownian motion plus a generalised compound Poisson process.

Similarly as before, for every measurable set $A$ with $\Pi(A)<\infty$ the process $X_A(t)$ of jumps of size in $A$ is a Poisson process with mean $\Pi(A) t.$ Hence the measure $\Pi$ is called the jump intensity measure.

\paragraph{Infinite divisibility:} It follows from the definition that the distribution of a L\'{e}vy  process at any time $t$ is inifinitely divisible. It means that for arbitrary $n$ it can be obtained as the sum of $n$ independent identically distributed random variables. The converse also holds: given an infinitely divisible distribution, then there exists a L\'{e}vy  process $X(t)$ for which $X(1)$ has this distribution. It can be used to construct L\'{e}vy  processes.

\begin{exercise}  Show that when $\Pi(\mathbb{R} \backslash \{0\})=\infty$, the process has almost surely infinitely many jumps over any time interval of positive length.
\end{exercise}

\begin{exercise}  Show that when \[ \int_{\mathbb{R}\backslash \{0\}} \min (1,x) \Pi(dx) < \infty ,\] we have
\[ \Phi_{X(1)}(\theta) = \exp \left( ci \theta -\frac{1}{2}\sigma^2 \theta^2 + \int_{\mathbb{R}\backslash \{0\}} (e^{i\theta x} -1) \Pi(dx) \right) ,\] for some constant $c.$
\end{exercise}

\section{Gamma process}

Consider a random variable $X$ with gamma distribution with mean $\mu$ and variance $\nu.$ Thus

\[ P(X\leq x)= \frac{1}{\Gamma(\mu^2/\nu)(\nu/\mu)^{\mu^2/\nu}} \int_0^x e^{-u\mu/\nu} u^{\mu^2/\nu-1} du.\]

and its characteristic function equals

\[ \Phi_X(\psi)=E(e^{i\psi X})=(1-i\psi \nu/\mu)^{-\mu^2/\nu}.\]

This distribution is infinitely divisible since for every $N$ we have

\[ (1-i\psi \nu/\mu)^{-\mu^2/\nu}=\left((1-i\psi (\nu/N)/(\mu/N))^{-(\mu/N)^2/(\nu/N)}\right)^N.\]

Hence we can construct the gamma process $\Gamma_\nu(t)$ as a L\'{e}vy  process such that its distribution at time $1$ is  a gamma distribution with mean $1$ and variance $\nu.$ It follows that the distribution of $\Gamma_\nu(t)$ is a gamma distribution with mean $t$ and variance  $\nu t$.

\begin{figure}[!ht]%
\centering
%trim option's parameter order: left bottom right top
\includegraphics[trim =0mm 0mm 0mm 0mm, clip,width=\textwidth]{FIGURES/gamma_5.jpg}%
\caption{5 sample paths of a Gamma process}%
\label{f:gamma_5}%
\end{figure}

\begin{figure}[!ht]%
\centering
%trim option's parameter order: left bottom right top
\includegraphics[trim =0mm 0mm 0mm 0mm, clip, width=\textwidth]{FIGURES/gamma_100.jpg}%
\caption{100 sample paths of a Gamma process}%
\label{f:gamma_100}%
\end{figure}


One may check that the L\'{e}vy  intensity measure equals

\[ \Pi(dx) = \frac{1}{\nu} x^{-1} e^{-x/\nu} dx.\]

Indeed,

\begin{eqnarray*}
\int_0^\infty \frac{1}{\nu} x^{-1} (e^{i\psi x} -1)e^{-x/\nu} dx &=& \frac{1}{\nu} \int_0^\infty  x^{-1} \sum_{n=1}^\infty \frac{(i\psi x)^n}{n!} e^{-x/\nu} dx \\
&=& \frac{1}{\nu} \sum_{n=1}^\infty \frac{(i\psi \nu)^n}{n} \\
&=& -\frac{1}{\nu} \ln (1-i\psi \nu). \\
\end{eqnarray*} 

Note that the Gamma process is not a straight compound Poisson process, since for every $\epsilon>0$ it has infinitely many jumps of size smaller than $\epsilon$ in any time interval of positive length.



\clearpage  
\section{Variance Gamma process}

We refer to \cite{vgOptionPricing} for more details about the variance gamma process.

The variance gamma process is obtained by replacing time $t$ in a Brownian motion with drift by $\Gamma_\nu(t):$

\[ X(t)=\theta \Gamma_\nu(t) + \sigma B(\Gamma_\nu(t)),\]

where $\Gamma_\nu(t)$ and $B(t)$ are independent.


\begin{figure}[ht]%
\centering
%trim option's parameter order: left bottom right top
\includegraphics[trim =0mm 0mm 0mm 0mm, clip, width=\textwidth]{FIGURES/vargamma_5.jpg}%
\caption{5 sample paths of a variance gamma process}%
\label{f:vargamma_5}%
\end{figure}


Since Brownian motion with drift and the Gamma process are independent L\'{e}vy  processes, the variance gamma process is a L\'{e}vy  process as well. Moreover, it is easy to compute its characteristic function:

\[ E(e^{i\psi X(t)})=\left(1-i(\psi \theta +i \frac{\sigma^2 \psi^2}{2})\nu \right)^{-\frac{1}{\nu}}.\]

Note that the quadratic variation of the Brownian motion $B(\Gamma_\nu(t))$ is the gamma process $\Gamma_\nu(t)$. Hence the name of the process.

When $\theta=0$ the process is symmetric. In finance one often uses the variance gamma process to model logarithmic returns of equity prices. In such cases one may take $\theta<0$ in order to get fatter left side tails, or to model the volatility skew.


\chapter{Time Changed L\'{e}vy  Processes}

\section{Chronometers}

The increments of L\'{e}vy  processes over disjoint time intervals of the same length are i.i.d. However, in stock markets we observe volatility clustering, meaning that in turmoil periods the distribution of logarithmic returns has larger variance than in calm periods. In more mathematical terms, the distribution of logarithmic returns is conditional on the available information. We can construct stochastic processes exhibiting volatility clustering by replacing the time $t$ in a given stochastic process $X(t)$ by an independent increasing process $\tau(t)$ starting at $0$. The result is a time-changed process

\[ Y(t)=X(\tau(t)).\]

We call the process $\tau(t)$ a chronometer. A simple example is a gamma process. However, by applying an independent gamma process as chronometer to a L\'{e}vy  process we do not obtain a process with volatility clustering. Indeed,the resulting process is again a L\'{e}vy  process. 

More interesting chronometers may be obtained by integrating a mean reverting process such as Vasicek's process:

\[ \tau(t)= \int_0^t x(s) ds  \]

where

\[ dx(s)=\kappa(\theta-x(s))+\sigma dB(s). \]

Here $x(s)$ can be interpreted as the speed of time. 

An interesting case arises when $X$ is a L\'{e}vy  process and the characteristic function of the chronometer is also known:

\[ \Phi_X(u,t)=\EV[]{e^{iuX(t)}} \]

\[ \Phi_\tau(u,t)=\EV[]{e^{iu\tau(t)}} \]

We then get

\begin{eqnarray*}
\Phi_Y(u,t)&=&\EV[]{e^{iuY(t)}} \\
&=&\EV[]{e^{iuX(\tau(t))}} \\
&=&\EV[]{\EV[]{e^{iuX(\tau(t))}|\tau(t)}} \\
&=&\EV[]{\Phi_X(u,\tau(t))} \\
&=&\EV[]{e^{\tau(t) \log \Phi_X(u,1)}} \\
&=&\Phi_\tau(-i  \log \Phi_X(u,1),t)
\end{eqnarray*}

In the next section we explain how to use this characteristic function to compute plain vanilla call prices.

\section{Pricing plain vanilla calls via characteristic functions}

In this section we follow \cite{FFT} quite closely.

Our goal is to price a plain vanilla call option on a non-dividend paying stock given the characteristic function of its risk neutral density at maturity (with respect to the bank account numeraire). Let $K=e^k$ denote the strike and $T$ the maturity. Denote the density at maturity in terms of $s=\log (S(T)/S(0))$ by $q(s),$  where $S(T)$ denotes the stock price at maturity. For sake of simplicity we assume a contant risk-free rate $r.$

The value of the call option then equals

\[ C(k)=\EV[]{e^{-r(T)} \max (0,e^s-e^k)}=\int_{k}^{\infty} e^{-rT} (e^s-e^k) q(s) ds.\]

We would like to take the Fourier transform 

\[ \int_{-\infty}^{\infty} e^{i v k} C(k) dk, \]

but to this end $C(k)$ needs to be square integrable. However, as $C(k)$ converges to $S(0)$ as $k \to -\infty$, it is not square integrable. For this reason we adapt $C(k)$ by setting 

\[ c(k)=C(k) e^{\alpha k}. \]

We will show in a moment that the Fourier transform

\[ \Psi(v)=\int_{-\infty}^{\infty} e^{i v k} c(k) dk \]

exists for some suitable $\alpha>0$. We have 

\begin{eqnarray*}
\Psi(v) &=& \int_{-\infty}^{\infty} e^{i v k} c(k) dk \\
&=&\int_{-\infty}^{\infty} e^{i v k} e^{\alpha k} \int_{k}^{\infty} e^{-rT} (e^s-e^k) q(s) ds dk \\
&=& \int_{-\infty}^{\infty} e^{-rT} q(s) \left(  \int_{-\infty}^{s} e^s  e^{(i v+\alpha) k} dk -  \int_{-\infty}^{s} e^{(i v+\alpha+1) k} dk \right) ds \\
&=&  e^{-rT} \int_{-\infty}^{\infty}  q(s) \left( \frac{e^{(i v+\alpha+1) s}}{iv+\alpha} -  \frac{e^{(i v+\alpha+1) s}}{iv+\alpha+1} \right) ds \\
&=& e^{-rT} \left(   \frac{\Phi(v-(\alpha+1)i)}{iv+\alpha} -  \frac{\Phi(v-(\alpha+1)i)}{iv+\alpha+1} \right) \\
&=& e^{-rT} \frac{\Phi(v-(\alpha+1)i)}{(iv+\alpha)(iv+\alpha+1)}
\end{eqnarray*}

where $\Phi(v)=\EV[]{e^{ivs}}$ denotes the characteristic function of the risk neutral density. Note that 
\[ |\Psi(v)|\leq |\Psi(0)|=e^{-rT}  \frac{\Phi(-(\alpha+1)i)}{\alpha(\alpha+1)},\]
so the Fourier transform exists if

\[ |\Phi(-(\alpha+1)i)|=|\EV{S(T)^{\alpha+1}}|<\infty. \]

Applying the inverse Fourier transform, we get

\begin{eqnarray*}
C(k)&=&e^{-\alpha k} c(k) \\
&=&e^{-\alpha k} \frac{1}{2\pi} \int_{-\infty}^{\infty} e^{-i v k} \Psi(v) dv \\
&=& e^{-\alpha k} \frac{e^{-rT}}{2\pi} \int_{-\infty}^{\infty} e^{-i v k} \frac{\Phi(v-(\alpha+1)i)}{(iv+\alpha)(iv+\alpha+1)} dv \\
&=& \frac{e^{k-rT}}{2\pi i} \int_{1+\alpha-\infty}^{1+\alpha+\infty} e^{-ku} \frac{\Phi(-iu)}{(u-1)u} du \\
&=&\frac{e^{k-rT}}{2\pi i} \left( \int_{\frac{1}{2}-i\infty}^{\frac{1}{2}+i\infty} e^{-ku} \frac{\Phi(-iu)}{(u-1)u} du + 2 \pi i e^{-k} \Phi(-i) \right) \\
&=& -Ke^{-rT} \frac{1}{2 \pi} \int_{-\infty}^{\infty} e^{-ku} \frac{\Phi(-iu)}{\frac{1}{4}+y^2} dy + S(0) \\
&=& S(0) - K e^{-rT} \int_0^1  e^{-ku} \Phi(-iu) dx 
\end{eqnarray*}

where we have used the following substitutions

\begin{eqnarray*}
u&=&\alpha+1+iv \\
u&=&\frac{1}{2}+iy \\
y&=&\frac{1}{2} \tan (\frac{\pi}{2} x)\\
\end{eqnarray*}

and we got rid of $\alpha$ by applying the residue theorem from complex analysis. Note that the integral in the obtained formula can easily be computed by numerical methods once the characteristic function is known in analytical form. 




 

 

 




 




 



  

\chapter{Appendices}


\section{Density function of the CIR short rate}

Let the CIR process be given by the SDE

$$ dr(t)=\kappa(\theta-r(t))dt+\sigma \sqrt{r(t)} dB(t).$$

We will now derive an analytic formula for its density based on a paper of Feller \cite{Feller}.  
 
Setting $a=\sigma^2/2$, $b=-\kappa$, $c=\kappa \theta$ and $x=r(t)$ the PDE for the density $\rho(r,t)$ of $r(t)$ at time $t$  can be written as

$$ \rho_t=-((bx+c)\rho)_x+(ax\rho)_{xx} $$

where we use subscript notation for partial derivatives.

To solve the PDE take the Laplace transform

$$ \omega(s,t)=\mathcal{L}(\rho)=\int_0^\infty e^{-sx} \rho(x,t) dx.$$

The PDE then transforms to 

\begin{eqnarray*}
\mathcal{L}(\rho_t)&=&\omega_t\\
&=&\mathcal{L}((-(bx+c)\rho+(ax\rho)_x)_x)+\lim_{x\to 0} ((bx+c)\rho-(ax\rho)x) \\
&=&s\left(as\mathcal{L}(x\rho)-b\mathcal{L}(x\rho)-c\mathcal{L}(\rho)\right) +f(t) \\
&=&s((-as+b)\omega_s -scw+f(t)
\end{eqnarray*}

We thus need to solve

$$ dt=\frac{ds}{s(as-b)}=\frac{d\omega}{f(t)-cs\omega} $$

The first equation can be solved by writing $$\frac{1}{s(as-b)}=\frac{a}{b(as-b)}-\frac{1}{bs} $$
and integrating out $s$. We get

$$e^{-bt}=C_1 \frac{s}{as-b}.$$

Substituting this in the second equation we need to solve

$$ \omega_t=f(t)-sc\omega=f(t)-c\frac{b}{a-C_1e^{bt}}\omega.$$

Write $$ \lambda=\omega g,$$ then

$$\frac{\lambda_t}{g}=\omega_t+\omega\frac{g_t}{g}$$

So by solving $$ \frac{g_t}{g}=\frac{cb}{a-C_1e^{bt}}$$

we get $$g=D|C_1-ae^{-bt}|^{-c/a},$$

for some constant $D$ and $$\lambda_t=f(t)g,$$

so $$\lambda=D\left( \int_0^t f(\tau) |C_1-ae^{-b\tau}|^{-c/a} d\tau + C_2 \right)$$

for some constant $C_2$ and hence

$$\omega= |C_1-ae^{-bt}|^{c/a} \left( \int_0^t f(\tau) |C_1-ae^{-b\tau}|^{-c/a} d\tau + C_2 \right) .$$

We now look for solutions $\rho(x,t)$ with initial condition equal to the Dirac measure in $r(0)$

$$\rho(x,0)=\delta_{r(0)}(x)$$

whose Laplace transform equals 

$$ \omega(s,0)=\pi(s)=e^{-r(0)s}.$$

Hence $$ \pi(s)=|C_1-a|^{c/a} C_2$$ from which

$$ C_2=\pi(s)|C_1-a|^{-c/a}= \pi(\frac{b}{a-C_1})|C_1-a|^{-c/a}.$$

We thus get 

\begin{eqnarray*}
\omega(s,t)&=&|e^{-bt}C_1-ae^{-bt}|^{c/a} \left( \int_0^t f(\tau) |C_1-ae^{-b\tau}|^{-c/a} d\tau + C_2 \right) \\
&=& |e^{-bt}C_1-ae^{-bt}|^{c/a} \left( \int_0^t f(\tau) |C_1-ae^{-b\tau}|^{-c/a} d\tau + \pi(\frac{b}{a-C_1})|C_1-a|^{-c/a} \right) \\
&=& |e^{-bt}\frac{b}{s}|^{c/a} \Big( \pi(\frac{b}{a(1-e^{-bt})+e^{-bt}\frac{b}{s}})|a(1-e^{-bt})+e^{-bt}\frac{b}{s}|^{-c/a} \\
&& +  \int_0^t f(\tau) |a(e^{-bt}-e^{-b\tau})-e^{-bt}\frac{b}{s}|^{-c/a} d\tau \Big)\\ 
&=& (\frac{b}{as(e^{bt}-1)+b})^{c/a} \pi(\frac{bse^{bt}}{as(e^{bt}-1)+b}) + \int_0^t f(\tau) (as(e^{b(t-\tau)}-1)+b)^{-c/a} d\tau 
\end{eqnarray*}

We now look for a solution with $f(t)=0.$ This can be obtained by taking the inverse Laplace transform.

\begin{eqnarray*}
u(r(t)&=&\frac{1}{2\pi i} \int_{-i\infty}^{i\infty} e^{sr(t)} \omega(s,t) ds \\
&=& \frac{1}{2\pi i} \int_{-i\infty}^{i\infty} e^{sr(t)} (\frac{b}{\gamma s+b})^{c/a} \exp \left(-r(0)\frac{bse^{bt}}{\gamma s+b} \right) ds \\
&=&\frac{1}{2\pi i} e^{-\frac{b}{\gamma} (r(t)+r(0)e^{bt})} (\frac{b}{\gamma})^{c/a} \int_{b/\gamma-i\infty}^{b/\gamma + i\infty} e^{zr(t)} z^{-c/a} e^{\alpha z^{-1}} dz \\
&=&\frac{1}{2\pi i} e^{-\frac{b}{\gamma} (r(t)+r(0)e^{bt})} (\frac{b}{\gamma})^{c/a}
(\frac{\alpha}{r(t)})^{-\frac{c}{2a}+\frac{1}{2}}
 \int_{b/\gamma \sqrt{\frac{r(t)}{\alpha}} -i\infty}^{b/\gamma \sqrt{\frac{r(t)}{\alpha}}+ i\infty} e^{\sqrt{\alpha r(t)}(w+\frac{1}{w})} w^{-c/a} dw \\
&=&=  \frac{1}{2\pi i} e^{-\frac{b}{\gamma} (r(t)+r(0)e^{bt})} (\frac{b}{\gamma})^{c/a}
(\frac{\alpha}{r(t)})^{-\frac{c}{2a}+\frac{1}{2}}
 I_q(2\sqrt{\alpha r(t)}) \\
 & = & d e^{-u-v} \left(\frac{v}{u}\right)^{\frac{q}{2}} I_q(2\sqrt{uv}) 
\end{eqnarray*}

where we used the following substitutions 

\begin{eqnarray*}
\gamma & =& a(e^{bt}-1)\\
z&=&s+\frac{b}{\gamma} \\
\alpha&=&r(0)e^{bt}(\frac{b}{\gamma})^2 \\
w&=&\sqrt{\frac{r(t)}{\alpha}}z\\
a&=&\sigma^2/2\\
b&=&-\kappa\\
c&=&\kappa \theta \\
d&=&\frac{2\kappa}{\sigma^2(1-e^{-\kappa(t)})} \\
u&=&dr(0)e^{-\kappa(t)} \\
v&=&dr(t) \\
q&=&\frac{2\kappa \theta}{\sigma^2}-1
\end{eqnarray*} 

and $I_q$ denotes the modified Bessel function of the first kind of order $q$.


\section{Solving the Feynman-Kac equation for zero coupon bond prices in a CIR model}
\label{appendix:FeynmanKac}

In the CIR model the prices of zero coupon bonds $P(t,T)$ can be obtained by solving the Feynman-Kac equation for the bond price.  As in Vasicek's model we assume an affine bond privce structure , i.e. 

$$ P(t,T) A(t,T) e^{-B(t,T) r(t)} $$

for smooth functions $A(t,T)$ and $B(t,T)$.
This yields the following system of ordinary PDEs

\begin{eqnarray*}
1&=&  - \frac{\partial B(t,T)}{\partial t} + B(t,T)\kappa + \frac{1}{2} \sigma^2 B(t,T)^2 \\
0&=& \frac{{\partial A(t,T)}/{\partial t}}{A(t,T)} -B(t,T)\kappa \theta 
\end{eqnarray*}

The first equation is a so-called Ricatti equation and can be solved by substituting

$$ B=\frac{2}{\sigma^2}v$$ and $$v=-\frac{u_t}{u},$$

where we used subscript notation for partial derivatives. 

This yields 

$$ u=C_1e^{\gamma_+ t}+C_2e^{\gamma_- t}$$

for some constants $C_1$ and $C_2$
where $$\gamma_{\pm}=\frac{\kappa\pm \psi}{2}$$
and $$ \psi= \sqrt{\kappa^2+2\sigma^2}.$$

It follows that 

$$B(t,T)=\frac{-e^{-\psi t}\frac{\kappa-\psi}{2}-C\frac{\kappa+\psi}{2}}{\frac{\sigma^2}{2}(e^{-\psi t}+C)}$$

where $C$ must be solved by requiring $B(T,T)=0$. Thus $$ C= \frac{\psi-\kappa}{\psi+\kappa}e^{-\psi T}$$

and

$$B(t,T)= \frac{2(e^{\psi (T-t)}-1)}{(\psi+\kappa)e^{\psi (T-t)}+\psi - \kappa}.$$

To solve $A(t,T)$ note that the second equation can be rewritten as

$$ \frac{\partial \ln A(t,T)}{\partial t}=\kappa \theta B(t,T),$$ 

and hence integrating out we get 

\begin{eqnarray*}
\ln A(t,T)& =&  \kappa \theta \int_T^t \frac{2(e^{\psi (T-s)}-1)}{(\psi+\kappa)e^{\psi (T-s)}+\psi - \kappa} ds + \ln C\\
&=& -\frac{2 \kappa \theta}{\psi} \int_1^{e^{\psi(T-t)}}\frac{u-1}{((\psi+\kappa)u+\psi-\kappa)u} du + \ln C\\
&=&-\frac{2 \kappa \theta}{\psi(\psi-\kappa)} \int_1^{e^{\psi(T-t)}} \left( \frac{1}{u}-\frac{2\psi}{(\psi+\kappa)u+\psi-\kappa} \right) du + \ln C\\
\end{eqnarray*}

Hence

$$ A(t,T)=D\left( \frac{e^{\psi(T-t)}}{\big((\psi+\kappa)e^{\psi(T-t)}+\psi-\kappa\big)^\frac{2\psi}{\psi+\kappa}}\right)^{-\frac{2\kappa\theta}{\psi(\psi+\kappa)}},$$

where the constant $D$ is determined by requiring $A(T,T)=1$. It follows that 

$$ D= (2\psi)^\frac{2\kappa \theta}{\sigma^2} $$

and

$$ A(t,T)= \left( \frac{2\psi e^{\frac{(\psi+\kappa)(T-t)}{2}}}{(\psi+\kappa)e^{\psi(T-t)}+\psi-\kappa} \right)^\frac{2\kappa \theta}{\sigma^2}.$$


\bibliographystyle{alpha}   
\bibliography{BIBLIOGRAPHY/BIBLIOGRAPHY}


\end{document}








